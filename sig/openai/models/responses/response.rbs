module OpenAI
  module Models
    module Responses
      type response =
        {
          id: String,
          created_at: Float,
          error: OpenAI::Responses::ResponseError?,
          incomplete_details: OpenAI::Responses::Response::IncompleteDetails?,
          instructions: OpenAI::Models::Responses::Response::instructions?,
          metadata: OpenAI::Models::metadata?,
          model: OpenAI::Models::responses_model,
          object: :response,
          output: ::Array[OpenAI::Models::Responses::response_output_item],
          parallel_tool_calls: bool,
          temperature: Float?,
          tool_choice: OpenAI::Models::Responses::Response::tool_choice,
          tools: ::Array[OpenAI::Models::Responses::tool],
          top_p: Float?,
          background: bool?,
          completed_at: Float?,
          conversation: OpenAI::Responses::Response::Conversation?,
          max_output_tokens: Integer?,
          max_tool_calls: Integer?,
          previous_response_id: String?,
          prompt: OpenAI::Responses::ResponsePrompt?,
          prompt_cache_key: String,
          prompt_cache_retention: OpenAI::Models::Responses::Response::prompt_cache_retention?,
          reasoning: OpenAI::Reasoning?,
          safety_identifier: String,
          service_tier: OpenAI::Models::Responses::Response::service_tier?,
          status: OpenAI::Models::Responses::response_status,
          text: OpenAI::Responses::ResponseTextConfig,
          top_logprobs: Integer?,
          truncation: OpenAI::Models::Responses::Response::truncation?,
          usage: OpenAI::Responses::ResponseUsage,
          user: String
        }

      class Response < OpenAI::Internal::Type::BaseModel
        attr_accessor id: String

        attr_accessor created_at: Float

        attr_accessor error: OpenAI::Responses::ResponseError?

        attr_accessor incomplete_details: OpenAI::Responses::Response::IncompleteDetails?

        attr_accessor instructions: OpenAI::Models::Responses::Response::instructions?

        attr_accessor metadata: OpenAI::Models::metadata?

        attr_accessor model: OpenAI::Models::responses_model

        attr_accessor object: :response

        attr_accessor output: ::Array[OpenAI::Models::Responses::response_output_item]

        attr_accessor parallel_tool_calls: bool

        attr_accessor temperature: Float?

        attr_accessor tool_choice: OpenAI::Models::Responses::Response::tool_choice

        attr_accessor tools: ::Array[OpenAI::Models::Responses::tool]

        attr_accessor top_p: Float?

        attr_accessor background: bool?

        attr_accessor completed_at: Float?

        attr_accessor conversation: OpenAI::Responses::Response::Conversation?

        attr_accessor max_output_tokens: Integer?

        attr_accessor max_tool_calls: Integer?

        attr_accessor previous_response_id: String?

        attr_accessor prompt: OpenAI::Responses::ResponsePrompt?

        attr_reader prompt_cache_key: String?

        def prompt_cache_key=: (String) -> String

        attr_accessor prompt_cache_retention: OpenAI::Models::Responses::Response::prompt_cache_retention?

        attr_accessor reasoning: OpenAI::Reasoning?

        attr_reader safety_identifier: String?

        def safety_identifier=: (String) -> String

        attr_accessor service_tier: OpenAI::Models::Responses::Response::service_tier?

        attr_reader status: OpenAI::Models::Responses::response_status?

        def status=: (
          OpenAI::Models::Responses::response_status
        ) -> OpenAI::Models::Responses::response_status

        attr_reader text: OpenAI::Responses::ResponseTextConfig?

        def text=: (
          OpenAI::Responses::ResponseTextConfig
        ) -> OpenAI::Responses::ResponseTextConfig

        attr_accessor top_logprobs: Integer?

        attr_accessor truncation: OpenAI::Models::Responses::Response::truncation?

        attr_reader usage: OpenAI::Responses::ResponseUsage?

        def usage=: (
          OpenAI::Responses::ResponseUsage
        ) -> OpenAI::Responses::ResponseUsage

        attr_reader user: String?

        def user=: (String) -> String

        def initialize: (
          id: String,
          created_at: Float,
          error: OpenAI::Responses::ResponseError?,
          incomplete_details: OpenAI::Responses::Response::IncompleteDetails?,
          instructions: OpenAI::Models::Responses::Response::instructions?,
          metadata: OpenAI::Models::metadata?,
          model: OpenAI::Models::responses_model,
          output: ::Array[OpenAI::Models::Responses::response_output_item],
          parallel_tool_calls: bool,
          temperature: Float?,
          tool_choice: OpenAI::Models::Responses::Response::tool_choice,
          tools: ::Array[OpenAI::Models::Responses::tool],
          top_p: Float?,
          ?background: bool?,
          ?completed_at: Float?,
          ?conversation: OpenAI::Responses::Response::Conversation?,
          ?max_output_tokens: Integer?,
          ?max_tool_calls: Integer?,
          ?previous_response_id: String?,
          ?prompt: OpenAI::Responses::ResponsePrompt?,
          ?prompt_cache_key: String,
          ?prompt_cache_retention: OpenAI::Models::Responses::Response::prompt_cache_retention?,
          ?reasoning: OpenAI::Reasoning?,
          ?safety_identifier: String,
          ?service_tier: OpenAI::Models::Responses::Response::service_tier?,
          ?status: OpenAI::Models::Responses::response_status,
          ?text: OpenAI::Responses::ResponseTextConfig,
          ?top_logprobs: Integer?,
          ?truncation: OpenAI::Models::Responses::Response::truncation?,
          ?usage: OpenAI::Responses::ResponseUsage,
          ?user: String,
          ?object: :response
        ) -> void

        def to_hash: -> {
          id: String,
          created_at: Float,
          error: OpenAI::Responses::ResponseError?,
          incomplete_details: OpenAI::Responses::Response::IncompleteDetails?,
          instructions: OpenAI::Models::Responses::Response::instructions?,
          metadata: OpenAI::Models::metadata?,
          model: OpenAI::Models::responses_model,
          object: :response,
          output: ::Array[OpenAI::Models::Responses::response_output_item],
          parallel_tool_calls: bool,
          temperature: Float?,
          tool_choice: OpenAI::Models::Responses::Response::tool_choice,
          tools: ::Array[OpenAI::Models::Responses::tool],
          top_p: Float?,
          background: bool?,
          completed_at: Float?,
          conversation: OpenAI::Responses::Response::Conversation?,
          max_output_tokens: Integer?,
          max_tool_calls: Integer?,
          previous_response_id: String?,
          prompt: OpenAI::Responses::ResponsePrompt?,
          prompt_cache_key: String,
          prompt_cache_retention: OpenAI::Models::Responses::Response::prompt_cache_retention?,
          reasoning: OpenAI::Reasoning?,
          safety_identifier: String,
          service_tier: OpenAI::Models::Responses::Response::service_tier?,
          status: OpenAI::Models::Responses::response_status,
          text: OpenAI::Responses::ResponseTextConfig,
          top_logprobs: Integer?,
          truncation: OpenAI::Models::Responses::Response::truncation?,
          usage: OpenAI::Responses::ResponseUsage,
          user: String
        }

        type incomplete_details =
          {
            reason: OpenAI::Models::Responses::Response::IncompleteDetails::reason
          }

        class IncompleteDetails < OpenAI::Internal::Type::BaseModel
          attr_reader reason: OpenAI::Models::Responses::Response::IncompleteDetails::reason?

          def reason=: (
            OpenAI::Models::Responses::Response::IncompleteDetails::reason
          ) -> OpenAI::Models::Responses::Response::IncompleteDetails::reason

          def initialize: (
            ?reason: OpenAI::Models::Responses::Response::IncompleteDetails::reason
          ) -> void

          def to_hash: -> {
            reason: OpenAI::Models::Responses::Response::IncompleteDetails::reason
          }

          type reason = :max_output_tokens | :content_filter

          module Reason
            extend OpenAI::Internal::Type::Enum

            MAX_OUTPUT_TOKENS: :max_output_tokens
            CONTENT_FILTER: :content_filter

            def self?.values: -> ::Array[OpenAI::Models::Responses::Response::IncompleteDetails::reason]
          end
        end

        type instructions =
          String | ::Array[OpenAI::Models::Responses::response_input_item]

        module Instructions
          extend OpenAI::Internal::Type::Union

          def self?.variants: -> ::Array[OpenAI::Models::Responses::Response::instructions]

          ResponseInputItemArray: OpenAI::Internal::Type::Converter
        end

        type tool_choice =
          OpenAI::Models::Responses::tool_choice_options
          | OpenAI::Responses::ToolChoiceAllowed
          | OpenAI::Responses::ToolChoiceTypes
          | OpenAI::Responses::ToolChoiceFunction
          | OpenAI::Responses::ToolChoiceMcp
          | OpenAI::Responses::ToolChoiceCustom
          | OpenAI::Responses::ToolChoiceApplyPatch
          | OpenAI::Responses::ToolChoiceShell

        module ToolChoice
          extend OpenAI::Internal::Type::Union

          def self?.variants: -> ::Array[OpenAI::Models::Responses::Response::tool_choice]
        end

        type conversation = { id: String }

        class Conversation < OpenAI::Internal::Type::BaseModel
          attr_accessor id: String

          def initialize: (id: String) -> void

          def to_hash: -> { id: String }
        end

        type prompt_cache_retention = :"in-memory" | :"24h"

        module PromptCacheRetention
          extend OpenAI::Internal::Type::Enum

          IN_MEMORY: :"in-memory"
          PROMPT_CACHE_RETENTION_24H: :"24h"

          def self?.values: -> ::Array[OpenAI::Models::Responses::Response::prompt_cache_retention]
        end

        type service_tier = :auto | :default | :flex | :scale | :priority

        module ServiceTier
          extend OpenAI::Internal::Type::Enum

          AUTO: :auto
          DEFAULT: :default
          FLEX: :flex
          SCALE: :scale
          PRIORITY: :priority

          def self?.values: -> ::Array[OpenAI::Models::Responses::Response::service_tier]
        end

        type truncation = :auto | :disabled

        module Truncation
          extend OpenAI::Internal::Type::Enum

          AUTO: :auto
          DISABLED: :disabled

          def self?.values: -> ::Array[OpenAI::Models::Responses::Response::truncation]
        end
      end
    end
  end
end
