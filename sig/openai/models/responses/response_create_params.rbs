module OpenAI
  module Models
    module Responses
      type response_create_params =
        {
          background: bool?,
          include: ::Array[OpenAI::Models::Responses::response_includable]?,
          input: OpenAI::Models::Responses::ResponseCreateParams::input,
          instructions: String?,
          max_output_tokens: Integer?,
          max_tool_calls: Integer?,
          metadata: OpenAI::Models::metadata?,
          model: OpenAI::Models::responses_model,
          parallel_tool_calls: bool?,
          previous_response_id: String?,
          prompt: OpenAI::Responses::ResponsePrompt?,
          prompt_cache_key: String,
          reasoning: OpenAI::Reasoning?,
          safety_identifier: String,
          service_tier: OpenAI::Models::Responses::ResponseCreateParams::service_tier?,
          store: bool?,
          stream_options: OpenAI::Responses::ResponseCreateParams::StreamOptions?,
          temperature: Float?,
          text: OpenAI::Responses::ResponseTextConfig,
          tool_choice: OpenAI::Models::Responses::ResponseCreateParams::tool_choice,
          tools: ::Array[OpenAI::Models::Responses::tool],
          top_logprobs: Integer?,
          top_p: Float?,
          truncation: OpenAI::Models::Responses::ResponseCreateParams::truncation?,
          user: String
        }
        & OpenAI::Internal::Type::request_parameters

      class ResponseCreateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        attr_accessor background: bool?

        attr_accessor include: ::Array[OpenAI::Models::Responses::response_includable]?

        attr_reader input: OpenAI::Models::Responses::ResponseCreateParams::input?

        def input=: (
          OpenAI::Models::Responses::ResponseCreateParams::input
        ) -> OpenAI::Models::Responses::ResponseCreateParams::input

        attr_accessor instructions: String?

        attr_accessor max_output_tokens: Integer?

        attr_accessor max_tool_calls: Integer?

        attr_accessor metadata: OpenAI::Models::metadata?

        attr_reader model: OpenAI::Models::responses_model?

        def model=: (
          OpenAI::Models::responses_model
        ) -> OpenAI::Models::responses_model

        attr_accessor parallel_tool_calls: bool?

        attr_accessor previous_response_id: String?

        attr_accessor prompt: OpenAI::Responses::ResponsePrompt?

        attr_reader prompt_cache_key: String?

        def prompt_cache_key=: (String) -> String

        attr_accessor reasoning: OpenAI::Reasoning?

        attr_reader safety_identifier: String?

        def safety_identifier=: (String) -> String

        attr_accessor service_tier: OpenAI::Models::Responses::ResponseCreateParams::service_tier?

        attr_accessor store: bool?

        attr_accessor stream_options: OpenAI::Responses::ResponseCreateParams::StreamOptions?

        attr_accessor temperature: Float?

        attr_reader text: OpenAI::Responses::ResponseTextConfig?

        def text=: (
          OpenAI::Responses::ResponseTextConfig
        ) -> OpenAI::Responses::ResponseTextConfig

        attr_reader tool_choice: OpenAI::Models::Responses::ResponseCreateParams::tool_choice?

        def tool_choice=: (
          OpenAI::Models::Responses::ResponseCreateParams::tool_choice
        ) -> OpenAI::Models::Responses::ResponseCreateParams::tool_choice

        attr_reader tools: ::Array[OpenAI::Models::Responses::tool]?

        def tools=: (
          ::Array[OpenAI::Models::Responses::tool]
        ) -> ::Array[OpenAI::Models::Responses::tool]

        attr_accessor top_logprobs: Integer?

        attr_accessor top_p: Float?

        attr_accessor truncation: OpenAI::Models::Responses::ResponseCreateParams::truncation?

        attr_reader user: String?

        def user=: (String) -> String

        def initialize: (
          ?background: bool?,
          ?include: ::Array[OpenAI::Models::Responses::response_includable]?,
          ?input: OpenAI::Models::Responses::ResponseCreateParams::input,
          ?instructions: String?,
          ?max_output_tokens: Integer?,
          ?max_tool_calls: Integer?,
          ?metadata: OpenAI::Models::metadata?,
          ?model: OpenAI::Models::responses_model,
          ?parallel_tool_calls: bool?,
          ?previous_response_id: String?,
          ?prompt: OpenAI::Responses::ResponsePrompt?,
          ?prompt_cache_key: String,
          ?reasoning: OpenAI::Reasoning?,
          ?safety_identifier: String,
          ?service_tier: OpenAI::Models::Responses::ResponseCreateParams::service_tier?,
          ?store: bool?,
          ?stream_options: OpenAI::Responses::ResponseCreateParams::StreamOptions?,
          ?temperature: Float?,
          ?text: OpenAI::Responses::ResponseTextConfig,
          ?tool_choice: OpenAI::Models::Responses::ResponseCreateParams::tool_choice,
          ?tools: ::Array[OpenAI::Models::Responses::tool],
          ?top_logprobs: Integer?,
          ?top_p: Float?,
          ?truncation: OpenAI::Models::Responses::ResponseCreateParams::truncation?,
          ?user: String,
          ?request_options: OpenAI::request_opts
        ) -> void

        def to_hash: -> {
          background: bool?,
          include: ::Array[OpenAI::Models::Responses::response_includable]?,
          input: OpenAI::Models::Responses::ResponseCreateParams::input,
          instructions: String?,
          max_output_tokens: Integer?,
          max_tool_calls: Integer?,
          metadata: OpenAI::Models::metadata?,
          model: OpenAI::Models::responses_model,
          parallel_tool_calls: bool?,
          previous_response_id: String?,
          prompt: OpenAI::Responses::ResponsePrompt?,
          prompt_cache_key: String,
          reasoning: OpenAI::Reasoning?,
          safety_identifier: String,
          service_tier: OpenAI::Models::Responses::ResponseCreateParams::service_tier?,
          store: bool?,
          stream_options: OpenAI::Responses::ResponseCreateParams::StreamOptions?,
          temperature: Float?,
          text: OpenAI::Responses::ResponseTextConfig,
          tool_choice: OpenAI::Models::Responses::ResponseCreateParams::tool_choice,
          tools: ::Array[OpenAI::Models::Responses::tool],
          top_logprobs: Integer?,
          top_p: Float?,
          truncation: OpenAI::Models::Responses::ResponseCreateParams::truncation?,
          user: String,
          request_options: OpenAI::RequestOptions
        }

        type input = String | OpenAI::Models::Responses::response_input

        module Input
          extend OpenAI::Internal::Type::Union

          def self?.variants: -> ::Array[OpenAI::Models::Responses::ResponseCreateParams::input]
        end

        type service_tier = :auto | :default | :flex | :scale | :priority

        module ServiceTier
          extend OpenAI::Internal::Type::Enum

          AUTO: :auto
          DEFAULT: :default
          FLEX: :flex
          SCALE: :scale
          PRIORITY: :priority

          def self?.values: -> ::Array[OpenAI::Models::Responses::ResponseCreateParams::service_tier]
        end

        type stream_options = { include_obfuscation: bool }

        class StreamOptions < OpenAI::Internal::Type::BaseModel
          attr_reader include_obfuscation: bool?

          def include_obfuscation=: (bool) -> bool

          def initialize: (?include_obfuscation: bool) -> void

          def to_hash: -> { include_obfuscation: bool }
        end

        type tool_choice =
          OpenAI::Models::Responses::tool_choice_options
          | OpenAI::Responses::ToolChoiceAllowed
          | OpenAI::Responses::ToolChoiceTypes
          | OpenAI::Responses::ToolChoiceFunction
          | OpenAI::Responses::ToolChoiceMcp
          | OpenAI::Responses::ToolChoiceCustom

        module ToolChoice
          extend OpenAI::Internal::Type::Union

          def self?.variants: -> ::Array[OpenAI::Models::Responses::ResponseCreateParams::tool_choice]
        end

        type truncation = :auto | :disabled

        module Truncation
          extend OpenAI::Internal::Type::Enum

          AUTO: :auto
          DISABLED: :disabled

          def self?.values: -> ::Array[OpenAI::Models::Responses::ResponseCreateParams::truncation]
        end
      end
    end
  end
end
