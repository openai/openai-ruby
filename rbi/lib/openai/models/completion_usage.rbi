# typed: strong

module OpenAI
  module Models
    class CompletionUsage < OpenAI::BaseModel
      # Number of tokens in the generated completion.
      sig { returns(Integer) }
      attr_accessor :completion_tokens

      # Number of tokens in the prompt.
      sig { returns(Integer) }
      attr_accessor :prompt_tokens

      # Total number of tokens used in the request (prompt + completion).
      sig { returns(Integer) }
      attr_accessor :total_tokens

      # Breakdown of tokens used in a completion.
      sig { returns(T.nilable(OpenAI::Models::CompletionUsage::CompletionTokensDetails)) }
      attr_reader :completion_tokens_details

      sig do
        params(
          completion_tokens_details: T.any(OpenAI::Models::CompletionUsage::CompletionTokensDetails, OpenAI::Internal::Util::AnyHash)
        )
          .void
      end
      attr_writer :completion_tokens_details

      # Breakdown of tokens used in the prompt.
      sig { returns(T.nilable(OpenAI::Models::CompletionUsage::PromptTokensDetails)) }
      attr_reader :prompt_tokens_details

      sig do
        params(
          prompt_tokens_details: T.any(OpenAI::Models::CompletionUsage::PromptTokensDetails, OpenAI::Internal::Util::AnyHash)
        )
          .void
      end
      attr_writer :prompt_tokens_details

      # Usage statistics for the completion request.
      sig do
        params(
          completion_tokens: Integer,
          prompt_tokens: Integer,
          total_tokens: Integer,
          completion_tokens_details: T.any(OpenAI::Models::CompletionUsage::CompletionTokensDetails, OpenAI::Internal::Util::AnyHash),
          prompt_tokens_details: T.any(OpenAI::Models::CompletionUsage::PromptTokensDetails, OpenAI::Internal::Util::AnyHash)
        )
          .returns(T.attached_class)
      end
      def self.new(
        completion_tokens:,
        prompt_tokens:,
        total_tokens:,
        completion_tokens_details: nil,
        prompt_tokens_details: nil
      )
      end

      sig do
        override
          .returns(
            {
              completion_tokens: Integer,
              prompt_tokens: Integer,
              total_tokens: Integer,
              completion_tokens_details: OpenAI::Models::CompletionUsage::CompletionTokensDetails,
              prompt_tokens_details: OpenAI::Models::CompletionUsage::PromptTokensDetails
            }
          )
      end
      def to_hash
      end

      class CompletionTokensDetails < OpenAI::BaseModel
        # When using Predicted Outputs, the number of tokens in the prediction that
        #   appeared in the completion.
        sig { returns(T.nilable(Integer)) }
        attr_reader :accepted_prediction_tokens

        sig { params(accepted_prediction_tokens: Integer).void }
        attr_writer :accepted_prediction_tokens

        # Audio input tokens generated by the model.
        sig { returns(T.nilable(Integer)) }
        attr_reader :audio_tokens

        sig { params(audio_tokens: Integer).void }
        attr_writer :audio_tokens

        # Tokens generated by the model for reasoning.
        sig { returns(T.nilable(Integer)) }
        attr_reader :reasoning_tokens

        sig { params(reasoning_tokens: Integer).void }
        attr_writer :reasoning_tokens

        # When using Predicted Outputs, the number of tokens in the prediction that did
        #   not appear in the completion. However, like reasoning tokens, these tokens are
        #   still counted in the total completion tokens for purposes of billing, output,
        #   and context window limits.
        sig { returns(T.nilable(Integer)) }
        attr_reader :rejected_prediction_tokens

        sig { params(rejected_prediction_tokens: Integer).void }
        attr_writer :rejected_prediction_tokens

        # Breakdown of tokens used in a completion.
        sig do
          params(
            accepted_prediction_tokens: Integer,
            audio_tokens: Integer,
            reasoning_tokens: Integer,
            rejected_prediction_tokens: Integer
          )
            .returns(T.attached_class)
        end
        def self.new(
          accepted_prediction_tokens: nil,
          audio_tokens: nil,
          reasoning_tokens: nil,
          rejected_prediction_tokens: nil
        )
        end

        sig do
          override
            .returns(
              {
                accepted_prediction_tokens: Integer,
                audio_tokens: Integer,
                reasoning_tokens: Integer,
                rejected_prediction_tokens: Integer
              }
            )
        end
        def to_hash
        end
      end

      class PromptTokensDetails < OpenAI::BaseModel
        # Audio input tokens present in the prompt.
        sig { returns(T.nilable(Integer)) }
        attr_reader :audio_tokens

        sig { params(audio_tokens: Integer).void }
        attr_writer :audio_tokens

        # Cached tokens present in the prompt.
        sig { returns(T.nilable(Integer)) }
        attr_reader :cached_tokens

        sig { params(cached_tokens: Integer).void }
        attr_writer :cached_tokens

        # Breakdown of tokens used in the prompt.
        sig { params(audio_tokens: Integer, cached_tokens: Integer).returns(T.attached_class) }
        def self.new(audio_tokens: nil, cached_tokens: nil)
        end

        sig { override.returns({audio_tokens: Integer, cached_tokens: Integer}) }
        def to_hash
        end
      end
    end
  end
end
