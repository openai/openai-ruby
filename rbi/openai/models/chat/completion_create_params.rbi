# typed: strong

module OpenAI
  module Models
    module Chat
      class CompletionCreateParams < OpenAI::Internal::Type::BaseModel
        extend OpenAI::Internal::Type::RequestParameters::Converter
        include OpenAI::Internal::Type::RequestParameters

        OrHash =
          T.type_alias do
            T.any(
              OpenAI::Chat::CompletionCreateParams,
              OpenAI::Internal::AnyHash
            )
          end

        # A list of messages comprising the conversation so far. Depending on the
        # [model](https://platform.openai.com/docs/models) you use, different message
        # types (modalities) are supported, like
        # [text](https://platform.openai.com/docs/guides/text-generation),
        # [images](https://platform.openai.com/docs/guides/vision), and
        # [audio](https://platform.openai.com/docs/guides/audio).
        sig do
          returns(
            T::Array[
              T.any(
                OpenAI::Chat::ChatCompletionDeveloperMessageParam,
                OpenAI::Chat::ChatCompletionSystemMessageParam,
                OpenAI::Chat::ChatCompletionUserMessageParam,
                OpenAI::Chat::ChatCompletionAssistantMessageParam,
                OpenAI::Chat::ChatCompletionToolMessageParam,
                OpenAI::Chat::ChatCompletionFunctionMessageParam
              )
            ]
          )
        end
        attr_accessor :messages

        # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
        # wide range of models with different capabilities, performance characteristics,
        # and price points. Refer to the
        # [model guide](https://platform.openai.com/docs/models) to browse and compare
        # available models.
        sig { returns(T.any(String, OpenAI::ChatModel::OrSymbol)) }
        attr_accessor :model

        # Parameters for audio output. Required when audio output is requested with
        # `modalities: ["audio"]`.
        # [Learn more](https://platform.openai.com/docs/guides/audio).
        sig { returns(T.nilable(OpenAI::Chat::ChatCompletionAudioParam)) }
        attr_reader :audio

        sig do
          params(
            audio: T.nilable(OpenAI::Chat::ChatCompletionAudioParam::OrHash)
          ).void
        end
        attr_writer :audio

        # Number between -2.0 and 2.0. Positive values penalize new tokens based on their
        # existing frequency in the text so far, decreasing the model's likelihood to
        # repeat the same line verbatim.
        sig { returns(T.nilable(Float)) }
        attr_accessor :frequency_penalty

        # Deprecated in favor of `tool_choice`.
        #
        # Controls which (if any) function is called by the model.
        #
        # `none` means the model will not call a function and instead generates a message.
        #
        # `auto` means the model can pick between generating a message or calling a
        # function.
        #
        # Specifying a particular function via `{"name": "my_function"}` forces the model
        # to call that function.
        #
        # `none` is the default when no functions are present. `auto` is the default if
        # functions are present.
        sig do
          returns(
            T.nilable(
              T.any(
                OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::OrSymbol,
                OpenAI::Chat::ChatCompletionFunctionCallOption
              )
            )
          )
        end
        attr_reader :function_call

        sig do
          params(
            function_call:
              T.any(
                OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::OrSymbol,
                OpenAI::Chat::ChatCompletionFunctionCallOption::OrHash
              )
          ).void
        end
        attr_writer :function_call

        # Deprecated in favor of `tools`.
        #
        # A list of functions the model may generate JSON inputs for.
        sig do
          returns(
            T.nilable(T::Array[OpenAI::Chat::CompletionCreateParams::Function])
          )
        end
        attr_reader :functions

        sig do
          params(
            functions:
              T::Array[OpenAI::Chat::CompletionCreateParams::Function::OrHash]
          ).void
        end
        attr_writer :functions

        # Modify the likelihood of specified tokens appearing in the completion.
        #
        # Accepts a JSON object that maps tokens (specified by their token ID in the
        # tokenizer) to an associated bias value from -100 to 100. Mathematically, the
        # bias is added to the logits generated by the model prior to sampling. The exact
        # effect will vary per model, but values between -1 and 1 should decrease or
        # increase likelihood of selection; values like -100 or 100 should result in a ban
        # or exclusive selection of the relevant token.
        sig { returns(T.nilable(T::Hash[Symbol, Integer])) }
        attr_accessor :logit_bias

        # Whether to return log probabilities of the output tokens or not. If true,
        # returns the log probabilities of each output token returned in the `content` of
        # `message`.
        sig { returns(T.nilable(T::Boolean)) }
        attr_accessor :logprobs

        # An upper bound for the number of tokens that can be generated for a completion,
        # including visible output tokens and
        # [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
        sig { returns(T.nilable(Integer)) }
        attr_accessor :max_completion_tokens

        # The maximum number of [tokens](/tokenizer) that can be generated in the chat
        # completion. This value can be used to control
        # [costs](https://openai.com/api/pricing/) for text generated via API.
        #
        # This value is now deprecated in favor of `max_completion_tokens`, and is not
        # compatible with
        # [o-series models](https://platform.openai.com/docs/guides/reasoning).
        sig { returns(T.nilable(Integer)) }
        attr_accessor :max_tokens

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # Output types that you would like the model to generate. Most models are capable
        # of generating text, which is the default:
        #
        # `["text"]`
        #
        # The `gpt-4o-audio-preview` model can also be used to
        # [generate audio](https://platform.openai.com/docs/guides/audio). To request that
        # this model generate both text and audio responses, you can use:
        #
        # `["text", "audio"]`
        sig do
          returns(
            T.nilable(
              T::Array[OpenAI::Chat::CompletionCreateParams::Modality::OrSymbol]
            )
          )
        end
        attr_accessor :modalities

        # How many chat completion choices to generate for each input message. Note that
        # you will be charged based on the number of generated tokens across all of the
        # choices. Keep `n` as `1` to minimize costs.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :n

        # Whether to enable
        # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
        # during tool use.
        sig { returns(T.nilable(T::Boolean)) }
        attr_reader :parallel_tool_calls

        sig { params(parallel_tool_calls: T::Boolean).void }
        attr_writer :parallel_tool_calls

        # Static predicted output content, such as the content of a text file that is
        # being regenerated.
        sig do
          returns(T.nilable(OpenAI::Chat::ChatCompletionPredictionContent))
        end
        attr_reader :prediction

        sig do
          params(
            prediction:
              T.nilable(OpenAI::Chat::ChatCompletionPredictionContent::OrHash)
          ).void
        end
        attr_writer :prediction

        # Number between -2.0 and 2.0. Positive values penalize new tokens based on
        # whether they appear in the text so far, increasing the model's likelihood to
        # talk about new topics.
        sig { returns(T.nilable(Float)) }
        attr_accessor :presence_penalty

        # **o-series models only**
        #
        # Constrains effort on reasoning for
        # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
        # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
        # result in faster responses and fewer tokens used on reasoning in a response.
        sig { returns(T.nilable(OpenAI::ReasoningEffort::OrSymbol)) }
        attr_accessor :reasoning_effort

        # An object specifying the format that the model must output.
        #
        # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
        # Outputs which ensures the model will match your supplied JSON schema. Learn more
        # in the
        # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
        #
        # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
        # ensures the message the model generates is valid JSON. Using `json_schema` is
        # preferred for models that support it.
        sig do
          returns(
            T.nilable(
              T.any(
                OpenAI::ResponseFormatText,
                OpenAI::ResponseFormatJSONSchema,
                OpenAI::StructuredOutput::JsonSchemaConverter,
                OpenAI::ResponseFormatJSONObject
              )
            )
          )
        end
        attr_reader :response_format

        sig do
          params(
            response_format:
              T.any(
                OpenAI::ResponseFormatText::OrHash,
                OpenAI::ResponseFormatJSONSchema::OrHash,
                OpenAI::StructuredOutput::JsonSchemaConverter,
                OpenAI::ResponseFormatJSONObject::OrHash
              )
          ).void
        end
        attr_writer :response_format

        # This feature is in Beta. If specified, our system will make a best effort to
        # sample deterministically, such that repeated requests with the same `seed` and
        # parameters should return the same result. Determinism is not guaranteed, and you
        # should refer to the `system_fingerprint` response parameter to monitor changes
        # in the backend.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :seed

        # Specifies the latency tier to use for processing the request. This parameter is
        # relevant for customers subscribed to the scale tier service:
        #
        # - If set to 'auto', and the Project is Scale tier enabled, the system will
        #   utilize scale tier credits until they are exhausted.
        # - If set to 'auto', and the Project is not Scale tier enabled, the request will
        #   be processed using the default service tier with a lower uptime SLA and no
        #   latency guarentee.
        # - If set to 'default', the request will be processed using the default service
        #   tier with a lower uptime SLA and no latency guarentee.
        # - If set to 'flex', the request will be processed with the Flex Processing
        #   service tier.
        #   [Learn more](https://platform.openai.com/docs/guides/flex-processing).
        # - When not set, the default behavior is 'auto'.
        #
        # When this parameter is set, the response body will include the `service_tier`
        # utilized.
        sig do
          returns(
            T.nilable(
              OpenAI::Chat::CompletionCreateParams::ServiceTier::OrSymbol
            )
          )
        end
        attr_accessor :service_tier

        # Not supported with latest reasoning models `o3` and `o4-mini`.
        #
        # Up to 4 sequences where the API will stop generating further tokens. The
        # returned text will not contain the stop sequence.
        sig do
          returns(
            T.nilable(OpenAI::Chat::CompletionCreateParams::Stop::Variants)
          )
        end
        attr_accessor :stop

        # Whether or not to store the output of this chat completion request for use in
        # our [model distillation](https://platform.openai.com/docs/guides/distillation)
        # or [evals](https://platform.openai.com/docs/guides/evals) products.
        sig { returns(T.nilable(T::Boolean)) }
        attr_accessor :store

        # Options for streaming response. Only set this when you set `stream: true`.
        sig { returns(T.nilable(OpenAI::Chat::ChatCompletionStreamOptions)) }
        attr_reader :stream_options

        sig do
          params(
            stream_options:
              T.nilable(OpenAI::Chat::ChatCompletionStreamOptions::OrHash)
          ).void
        end
        attr_writer :stream_options

        # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
        # make the output more random, while lower values like 0.2 will make it more
        # focused and deterministic. We generally recommend altering this or `top_p` but
        # not both.
        sig { returns(T.nilable(Float)) }
        attr_accessor :temperature

        # Controls which (if any) tool is called by the model. `none` means the model will
        # not call any tool and instead generates a message. `auto` means the model can
        # pick between generating a message or calling one or more tools. `required` means
        # the model must call one or more tools. Specifying a particular tool via
        # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
        # call that tool.
        #
        # `none` is the default when no tools are present. `auto` is the default if tools
        # are present.
        sig do
          returns(
            T.nilable(
              T.any(
                OpenAI::Chat::ChatCompletionToolChoiceOption::Auto::OrSymbol,
                OpenAI::Chat::ChatCompletionNamedToolChoice
              )
            )
          )
        end
        attr_reader :tool_choice

        sig do
          params(
            tool_choice:
              T.any(
                OpenAI::Chat::ChatCompletionToolChoiceOption::Auto::OrSymbol,
                OpenAI::Chat::ChatCompletionNamedToolChoice::OrHash
              )
          ).void
        end
        attr_writer :tool_choice

        # A list of tools the model may call. Currently, only functions are supported as a
        # tool. Use this to provide a list of functions the model may generate JSON inputs
        # for. A max of 128 functions are supported.
        sig { returns(T.nilable(T::Array[OpenAI::Chat::ChatCompletionTool])) }
        attr_reader :tools

        sig do
          params(
            tools:
              T::Array[
                T.any(
                  OpenAI::Chat::ChatCompletionTool::OrHash,
                  OpenAI::StructuredOutput::JsonSchemaConverter
                )
              ]
          ).void
        end
        attr_writer :tools

        # An integer between 0 and 20 specifying the number of most likely tokens to
        # return at each token position, each with an associated log probability.
        # `logprobs` must be set to `true` if this parameter is used.
        sig { returns(T.nilable(Integer)) }
        attr_accessor :top_logprobs

        # An alternative to sampling with temperature, called nucleus sampling, where the
        # model considers the results of the tokens with top_p probability mass. So 0.1
        # means only the tokens comprising the top 10% probability mass are considered.
        #
        # We generally recommend altering this or `temperature` but not both.
        sig { returns(T.nilable(Float)) }
        attr_accessor :top_p

        # A stable identifier for your end-users. Used to boost cache hit rates by better
        # bucketing similar requests and to help OpenAI detect and prevent abuse.
        # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
        sig { returns(T.nilable(String)) }
        attr_reader :user

        sig { params(user: String).void }
        attr_writer :user

        # This tool searches the web for relevant results to use in a response. Learn more
        # about the
        # [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
        sig do
          returns(
            T.nilable(OpenAI::Chat::CompletionCreateParams::WebSearchOptions)
          )
        end
        attr_reader :web_search_options

        sig do
          params(
            web_search_options:
              OpenAI::Chat::CompletionCreateParams::WebSearchOptions::OrHash
          ).void
        end
        attr_writer :web_search_options

        sig do
          params(
            messages:
              T::Array[
                T.any(
                  OpenAI::Chat::ChatCompletionDeveloperMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionSystemMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionUserMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionAssistantMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionToolMessageParam::OrHash,
                  OpenAI::Chat::ChatCompletionFunctionMessageParam::OrHash
                )
              ],
            model: T.any(String, OpenAI::ChatModel::OrSymbol),
            audio: T.nilable(OpenAI::Chat::ChatCompletionAudioParam::OrHash),
            frequency_penalty: T.nilable(Float),
            function_call:
              T.any(
                OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::OrSymbol,
                OpenAI::Chat::ChatCompletionFunctionCallOption::OrHash
              ),
            functions:
              T::Array[OpenAI::Chat::CompletionCreateParams::Function::OrHash],
            logit_bias: T.nilable(T::Hash[Symbol, Integer]),
            logprobs: T.nilable(T::Boolean),
            max_completion_tokens: T.nilable(Integer),
            max_tokens: T.nilable(Integer),
            metadata: T.nilable(T::Hash[Symbol, String]),
            modalities:
              T.nilable(
                T::Array[
                  OpenAI::Chat::CompletionCreateParams::Modality::OrSymbol
                ]
              ),
            n: T.nilable(Integer),
            parallel_tool_calls: T::Boolean,
            prediction:
              T.nilable(OpenAI::Chat::ChatCompletionPredictionContent::OrHash),
            presence_penalty: T.nilable(Float),
            reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
            response_format:
              T.any(
                OpenAI::ResponseFormatText::OrHash,
                OpenAI::ResponseFormatJSONSchema::OrHash,
                OpenAI::StructuredOutput::JsonSchemaConverter,
                OpenAI::ResponseFormatJSONObject::OrHash
              ),
            seed: T.nilable(Integer),
            service_tier:
              T.nilable(
                OpenAI::Chat::CompletionCreateParams::ServiceTier::OrSymbol
              ),
            stop:
              T.nilable(OpenAI::Chat::CompletionCreateParams::Stop::Variants),
            store: T.nilable(T::Boolean),
            stream_options:
              T.nilable(OpenAI::Chat::ChatCompletionStreamOptions::OrHash),
            temperature: T.nilable(Float),
            tool_choice:
              T.any(
                OpenAI::Chat::ChatCompletionToolChoiceOption::Auto::OrSymbol,
                OpenAI::Chat::ChatCompletionNamedToolChoice::OrHash
              ),
            tools:
              T::Array[
                T.any(
                  OpenAI::Chat::ChatCompletionTool::OrHash,
                  OpenAI::StructuredOutput::JsonSchemaConverter
                )
              ],
            top_logprobs: T.nilable(Integer),
            top_p: T.nilable(Float),
            user: String,
            web_search_options:
              OpenAI::Chat::CompletionCreateParams::WebSearchOptions::OrHash,
            request_options: OpenAI::RequestOptions::OrHash
          ).returns(T.attached_class)
        end
        def self.new(
          # A list of messages comprising the conversation so far. Depending on the
          # [model](https://platform.openai.com/docs/models) you use, different message
          # types (modalities) are supported, like
          # [text](https://platform.openai.com/docs/guides/text-generation),
          # [images](https://platform.openai.com/docs/guides/vision), and
          # [audio](https://platform.openai.com/docs/guides/audio).
          messages:,
          # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
          # wide range of models with different capabilities, performance characteristics,
          # and price points. Refer to the
          # [model guide](https://platform.openai.com/docs/models) to browse and compare
          # available models.
          model:,
          # Parameters for audio output. Required when audio output is requested with
          # `modalities: ["audio"]`.
          # [Learn more](https://platform.openai.com/docs/guides/audio).
          audio: nil,
          # Number between -2.0 and 2.0. Positive values penalize new tokens based on their
          # existing frequency in the text so far, decreasing the model's likelihood to
          # repeat the same line verbatim.
          frequency_penalty: nil,
          # Deprecated in favor of `tool_choice`.
          #
          # Controls which (if any) function is called by the model.
          #
          # `none` means the model will not call a function and instead generates a message.
          #
          # `auto` means the model can pick between generating a message or calling a
          # function.
          #
          # Specifying a particular function via `{"name": "my_function"}` forces the model
          # to call that function.
          #
          # `none` is the default when no functions are present. `auto` is the default if
          # functions are present.
          function_call: nil,
          # Deprecated in favor of `tools`.
          #
          # A list of functions the model may generate JSON inputs for.
          functions: nil,
          # Modify the likelihood of specified tokens appearing in the completion.
          #
          # Accepts a JSON object that maps tokens (specified by their token ID in the
          # tokenizer) to an associated bias value from -100 to 100. Mathematically, the
          # bias is added to the logits generated by the model prior to sampling. The exact
          # effect will vary per model, but values between -1 and 1 should decrease or
          # increase likelihood of selection; values like -100 or 100 should result in a ban
          # or exclusive selection of the relevant token.
          logit_bias: nil,
          # Whether to return log probabilities of the output tokens or not. If true,
          # returns the log probabilities of each output token returned in the `content` of
          # `message`.
          logprobs: nil,
          # An upper bound for the number of tokens that can be generated for a completion,
          # including visible output tokens and
          # [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
          max_completion_tokens: nil,
          # The maximum number of [tokens](/tokenizer) that can be generated in the chat
          # completion. This value can be used to control
          # [costs](https://openai.com/api/pricing/) for text generated via API.
          #
          # This value is now deprecated in favor of `max_completion_tokens`, and is not
          # compatible with
          # [o-series models](https://platform.openai.com/docs/guides/reasoning).
          max_tokens: nil,
          # Set of 16 key-value pairs that can be attached to an object. This can be useful
          # for storing additional information about the object in a structured format, and
          # querying for objects via API or the dashboard.
          #
          # Keys are strings with a maximum length of 64 characters. Values are strings with
          # a maximum length of 512 characters.
          metadata: nil,
          # Output types that you would like the model to generate. Most models are capable
          # of generating text, which is the default:
          #
          # `["text"]`
          #
          # The `gpt-4o-audio-preview` model can also be used to
          # [generate audio](https://platform.openai.com/docs/guides/audio). To request that
          # this model generate both text and audio responses, you can use:
          #
          # `["text", "audio"]`
          modalities: nil,
          # How many chat completion choices to generate for each input message. Note that
          # you will be charged based on the number of generated tokens across all of the
          # choices. Keep `n` as `1` to minimize costs.
          n: nil,
          # Whether to enable
          # [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
          # during tool use.
          parallel_tool_calls: nil,
          # Static predicted output content, such as the content of a text file that is
          # being regenerated.
          prediction: nil,
          # Number between -2.0 and 2.0. Positive values penalize new tokens based on
          # whether they appear in the text so far, increasing the model's likelihood to
          # talk about new topics.
          presence_penalty: nil,
          # **o-series models only**
          #
          # Constrains effort on reasoning for
          # [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
          # supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
          # result in faster responses and fewer tokens used on reasoning in a response.
          reasoning_effort: nil,
          # An object specifying the format that the model must output.
          #
          # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
          # Outputs which ensures the model will match your supplied JSON schema. Learn more
          # in the
          # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
          #
          # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
          # ensures the message the model generates is valid JSON. Using `json_schema` is
          # preferred for models that support it.
          response_format: nil,
          # This feature is in Beta. If specified, our system will make a best effort to
          # sample deterministically, such that repeated requests with the same `seed` and
          # parameters should return the same result. Determinism is not guaranteed, and you
          # should refer to the `system_fingerprint` response parameter to monitor changes
          # in the backend.
          seed: nil,
          # Specifies the latency tier to use for processing the request. This parameter is
          # relevant for customers subscribed to the scale tier service:
          #
          # - If set to 'auto', and the Project is Scale tier enabled, the system will
          #   utilize scale tier credits until they are exhausted.
          # - If set to 'auto', and the Project is not Scale tier enabled, the request will
          #   be processed using the default service tier with a lower uptime SLA and no
          #   latency guarentee.
          # - If set to 'default', the request will be processed using the default service
          #   tier with a lower uptime SLA and no latency guarentee.
          # - If set to 'flex', the request will be processed with the Flex Processing
          #   service tier.
          #   [Learn more](https://platform.openai.com/docs/guides/flex-processing).
          # - When not set, the default behavior is 'auto'.
          #
          # When this parameter is set, the response body will include the `service_tier`
          # utilized.
          service_tier: nil,
          # Not supported with latest reasoning models `o3` and `o4-mini`.
          #
          # Up to 4 sequences where the API will stop generating further tokens. The
          # returned text will not contain the stop sequence.
          stop: nil,
          # Whether or not to store the output of this chat completion request for use in
          # our [model distillation](https://platform.openai.com/docs/guides/distillation)
          # or [evals](https://platform.openai.com/docs/guides/evals) products.
          store: nil,
          # Options for streaming response. Only set this when you set `stream: true`.
          stream_options: nil,
          # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
          # make the output more random, while lower values like 0.2 will make it more
          # focused and deterministic. We generally recommend altering this or `top_p` but
          # not both.
          temperature: nil,
          # Controls which (if any) tool is called by the model. `none` means the model will
          # not call any tool and instead generates a message. `auto` means the model can
          # pick between generating a message or calling one or more tools. `required` means
          # the model must call one or more tools. Specifying a particular tool via
          # `{"type": "function", "function": {"name": "my_function"}}` forces the model to
          # call that tool.
          #
          # `none` is the default when no tools are present. `auto` is the default if tools
          # are present.
          tool_choice: nil,
          # A list of tools the model may call. Currently, only functions are supported as a
          # tool. Use this to provide a list of functions the model may generate JSON inputs
          # for. A max of 128 functions are supported.
          tools: nil,
          # An integer between 0 and 20 specifying the number of most likely tokens to
          # return at each token position, each with an associated log probability.
          # `logprobs` must be set to `true` if this parameter is used.
          top_logprobs: nil,
          # An alternative to sampling with temperature, called nucleus sampling, where the
          # model considers the results of the tokens with top_p probability mass. So 0.1
          # means only the tokens comprising the top 10% probability mass are considered.
          #
          # We generally recommend altering this or `temperature` but not both.
          top_p: nil,
          # A stable identifier for your end-users. Used to boost cache hit rates by better
          # bucketing similar requests and to help OpenAI detect and prevent abuse.
          # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
          user: nil,
          # This tool searches the web for relevant results to use in a response. Learn more
          # about the
          # [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
          web_search_options: nil,
          request_options: {}
        )
        end

        sig do
          override.returns(
            {
              messages:
                T::Array[
                  T.any(
                    OpenAI::Chat::ChatCompletionDeveloperMessageParam,
                    OpenAI::Chat::ChatCompletionSystemMessageParam,
                    OpenAI::Chat::ChatCompletionUserMessageParam,
                    OpenAI::Chat::ChatCompletionAssistantMessageParam,
                    OpenAI::Chat::ChatCompletionToolMessageParam,
                    OpenAI::Chat::ChatCompletionFunctionMessageParam
                  )
                ],
              model: T.any(String, OpenAI::ChatModel::OrSymbol),
              audio: T.nilable(OpenAI::Chat::ChatCompletionAudioParam),
              frequency_penalty: T.nilable(Float),
              function_call:
                T.any(
                  OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::OrSymbol,
                  OpenAI::Chat::ChatCompletionFunctionCallOption
                ),
              functions:
                T::Array[OpenAI::Chat::CompletionCreateParams::Function],
              logit_bias: T.nilable(T::Hash[Symbol, Integer]),
              logprobs: T.nilable(T::Boolean),
              max_completion_tokens: T.nilable(Integer),
              max_tokens: T.nilable(Integer),
              metadata: T.nilable(T::Hash[Symbol, String]),
              modalities:
                T.nilable(
                  T::Array[
                    OpenAI::Chat::CompletionCreateParams::Modality::OrSymbol
                  ]
                ),
              n: T.nilable(Integer),
              parallel_tool_calls: T::Boolean,
              prediction:
                T.nilable(OpenAI::Chat::ChatCompletionPredictionContent),
              presence_penalty: T.nilable(Float),
              reasoning_effort: T.nilable(OpenAI::ReasoningEffort::OrSymbol),
              response_format:
                T.any(
                  OpenAI::ResponseFormatText,
                  OpenAI::ResponseFormatJSONSchema,
                  OpenAI::ResponseFormatJSONObject
                ),
              seed: T.nilable(Integer),
              service_tier:
                T.nilable(
                  OpenAI::Chat::CompletionCreateParams::ServiceTier::OrSymbol
                ),
              stop:
                T.nilable(OpenAI::Chat::CompletionCreateParams::Stop::Variants),
              store: T.nilable(T::Boolean),
              stream_options:
                T.nilable(OpenAI::Chat::ChatCompletionStreamOptions),
              temperature: T.nilable(Float),
              tool_choice:
                T.any(
                  OpenAI::Chat::ChatCompletionToolChoiceOption::Auto::OrSymbol,
                  OpenAI::Chat::ChatCompletionNamedToolChoice
                ),
              tools:
                T::Array[
                  T.any(
                    OpenAI::Chat::ChatCompletionTool,
                    OpenAI::StructuredOutput::JsonSchemaConverter
                  )
                ],
              top_logprobs: T.nilable(Integer),
              top_p: T.nilable(Float),
              user: String,
              web_search_options:
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions,
              request_options: OpenAI::RequestOptions
            }
          )
        end
        def to_hash
        end

        # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
        # wide range of models with different capabilities, performance characteristics,
        # and price points. Refer to the
        # [model guide](https://platform.openai.com/docs/models) to browse and compare
        # available models.
        module Model
          extend OpenAI::Internal::Type::Union

          Variants =
            T.type_alias { T.any(String, OpenAI::ChatModel::TaggedSymbol) }

          sig do
            override.returns(
              T::Array[OpenAI::Chat::CompletionCreateParams::Model::Variants]
            )
          end
          def self.variants
          end
        end

        # Deprecated in favor of `tool_choice`.
        #
        # Controls which (if any) function is called by the model.
        #
        # `none` means the model will not call a function and instead generates a message.
        #
        # `auto` means the model can pick between generating a message or calling a
        # function.
        #
        # Specifying a particular function via `{"name": "my_function"}` forces the model
        # to call that function.
        #
        # `none` is the default when no functions are present. `auto` is the default if
        # functions are present.
        module FunctionCall
          extend OpenAI::Internal::Type::Union

          Variants =
            T.type_alias do
              T.any(
                OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::TaggedSymbol,
                OpenAI::Chat::ChatCompletionFunctionCallOption
              )
            end

          # `none` means the model will not call a function and instead generates a message.
          # `auto` means the model can pick between generating a message or calling a
          # function.
          module FunctionCallMode
            extend OpenAI::Internal::Type::Enum

            TaggedSymbol =
              T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode
                )
              end
            OrSymbol = T.type_alias { T.any(Symbol, String) }

            NONE =
              T.let(
                :none,
                OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::TaggedSymbol
              )
            AUTO =
              T.let(
                :auto,
                OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::TaggedSymbol
              )

            sig do
              override.returns(
                T::Array[
                  OpenAI::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode::TaggedSymbol
                ]
              )
            end
            def self.values
            end
          end

          sig do
            override.returns(
              T::Array[
                OpenAI::Chat::CompletionCreateParams::FunctionCall::Variants
              ]
            )
          end
          def self.variants
          end
        end

        class Function < OpenAI::Internal::Type::BaseModel
          OrHash =
            T.type_alias do
              T.any(
                OpenAI::Chat::CompletionCreateParams::Function,
                OpenAI::Internal::AnyHash
              )
            end

          # The name of the function to be called. Must be a-z, A-Z, 0-9, or contain
          # underscores and dashes, with a maximum length of 64.
          sig { returns(String) }
          attr_accessor :name

          # A description of what the function does, used by the model to choose when and
          # how to call the function.
          sig { returns(T.nilable(String)) }
          attr_reader :description

          sig { params(description: String).void }
          attr_writer :description

          # The parameters the functions accepts, described as a JSON Schema object. See the
          # [guide](https://platform.openai.com/docs/guides/function-calling) for examples,
          # and the
          # [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for
          # documentation about the format.
          #
          # Omitting `parameters` defines a function with an empty parameter list.
          sig { returns(T.nilable(T::Hash[Symbol, T.anything])) }
          attr_reader :parameters

          sig { params(parameters: T::Hash[Symbol, T.anything]).void }
          attr_writer :parameters

          sig do
            params(
              name: String,
              description: String,
              parameters: T::Hash[Symbol, T.anything]
            ).returns(T.attached_class)
          end
          def self.new(
            # The name of the function to be called. Must be a-z, A-Z, 0-9, or contain
            # underscores and dashes, with a maximum length of 64.
            name:,
            # A description of what the function does, used by the model to choose when and
            # how to call the function.
            description: nil,
            # The parameters the functions accepts, described as a JSON Schema object. See the
            # [guide](https://platform.openai.com/docs/guides/function-calling) for examples,
            # and the
            # [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for
            # documentation about the format.
            #
            # Omitting `parameters` defines a function with an empty parameter list.
            parameters: nil
          )
          end

          sig do
            override.returns(
              {
                name: String,
                description: String,
                parameters: T::Hash[Symbol, T.anything]
              }
            )
          end
          def to_hash
          end
        end

        module Modality
          extend OpenAI::Internal::Type::Enum

          TaggedSymbol =
            T.type_alias do
              T.all(Symbol, OpenAI::Chat::CompletionCreateParams::Modality)
            end
          OrSymbol = T.type_alias { T.any(Symbol, String) }

          TEXT =
            T.let(
              :text,
              OpenAI::Chat::CompletionCreateParams::Modality::TaggedSymbol
            )
          AUDIO =
            T.let(
              :audio,
              OpenAI::Chat::CompletionCreateParams::Modality::TaggedSymbol
            )

          sig do
            override.returns(
              T::Array[
                OpenAI::Chat::CompletionCreateParams::Modality::TaggedSymbol
              ]
            )
          end
          def self.values
          end
        end

        # An object specifying the format that the model must output.
        #
        # Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
        # Outputs which ensures the model will match your supplied JSON schema. Learn more
        # in the
        # [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
        #
        # Setting to `{ "type": "json_object" }` enables the older JSON mode, which
        # ensures the message the model generates is valid JSON. Using `json_schema` is
        # preferred for models that support it.
        module ResponseFormat
          extend OpenAI::Internal::Type::Union

          Variants =
            T.type_alias do
              T.any(
                OpenAI::ResponseFormatText,
                OpenAI::ResponseFormatJSONSchema,
                OpenAI::StructuredOutput::JsonSchemaConverter,
                OpenAI::ResponseFormatJSONObject
              )
            end

          sig do
            override.returns(
              T::Array[
                OpenAI::Chat::CompletionCreateParams::ResponseFormat::Variants
              ]
            )
          end
          def self.variants
          end
        end

        # Specifies the latency tier to use for processing the request. This parameter is
        # relevant for customers subscribed to the scale tier service:
        #
        # - If set to 'auto', and the Project is Scale tier enabled, the system will
        #   utilize scale tier credits until they are exhausted.
        # - If set to 'auto', and the Project is not Scale tier enabled, the request will
        #   be processed using the default service tier with a lower uptime SLA and no
        #   latency guarentee.
        # - If set to 'default', the request will be processed using the default service
        #   tier with a lower uptime SLA and no latency guarentee.
        # - If set to 'flex', the request will be processed with the Flex Processing
        #   service tier.
        #   [Learn more](https://platform.openai.com/docs/guides/flex-processing).
        # - When not set, the default behavior is 'auto'.
        #
        # When this parameter is set, the response body will include the `service_tier`
        # utilized.
        module ServiceTier
          extend OpenAI::Internal::Type::Enum

          TaggedSymbol =
            T.type_alias do
              T.all(Symbol, OpenAI::Chat::CompletionCreateParams::ServiceTier)
            end
          OrSymbol = T.type_alias { T.any(Symbol, String) }

          AUTO =
            T.let(
              :auto,
              OpenAI::Chat::CompletionCreateParams::ServiceTier::TaggedSymbol
            )
          DEFAULT =
            T.let(
              :default,
              OpenAI::Chat::CompletionCreateParams::ServiceTier::TaggedSymbol
            )
          FLEX =
            T.let(
              :flex,
              OpenAI::Chat::CompletionCreateParams::ServiceTier::TaggedSymbol
            )

          sig do
            override.returns(
              T::Array[
                OpenAI::Chat::CompletionCreateParams::ServiceTier::TaggedSymbol
              ]
            )
          end
          def self.values
          end
        end

        # Not supported with latest reasoning models `o3` and `o4-mini`.
        #
        # Up to 4 sequences where the API will stop generating further tokens. The
        # returned text will not contain the stop sequence.
        module Stop
          extend OpenAI::Internal::Type::Union

          Variants = T.type_alias { T.nilable(T.any(String, T::Array[String])) }

          sig do
            override.returns(
              T::Array[OpenAI::Chat::CompletionCreateParams::Stop::Variants]
            )
          end
          def self.variants
          end

          StringArray =
            T.let(
              OpenAI::Internal::Type::ArrayOf[String],
              OpenAI::Internal::Type::Converter
            )
        end

        class WebSearchOptions < OpenAI::Internal::Type::BaseModel
          OrHash =
            T.type_alias do
              T.any(
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions,
                OpenAI::Internal::AnyHash
              )
            end

          # High level guidance for the amount of context window space to use for the
          # search. One of `low`, `medium`, or `high`. `medium` is the default.
          sig do
            returns(
              T.nilable(
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::OrSymbol
              )
            )
          end
          attr_reader :search_context_size

          sig do
            params(
              search_context_size:
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::OrSymbol
            ).void
          end
          attr_writer :search_context_size

          # Approximate location parameters for the search.
          sig do
            returns(
              T.nilable(
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation
              )
            )
          end
          attr_reader :user_location

          sig do
            params(
              user_location:
                T.nilable(
                  OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::OrHash
                )
            ).void
          end
          attr_writer :user_location

          # This tool searches the web for relevant results to use in a response. Learn more
          # about the
          # [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
          sig do
            params(
              search_context_size:
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::OrSymbol,
              user_location:
                T.nilable(
                  OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::OrHash
                )
            ).returns(T.attached_class)
          end
          def self.new(
            # High level guidance for the amount of context window space to use for the
            # search. One of `low`, `medium`, or `high`. `medium` is the default.
            search_context_size: nil,
            # Approximate location parameters for the search.
            user_location: nil
          )
          end

          sig do
            override.returns(
              {
                search_context_size:
                  OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::OrSymbol,
                user_location:
                  T.nilable(
                    OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation
                  )
              }
            )
          end
          def to_hash
          end

          # High level guidance for the amount of context window space to use for the
          # search. One of `low`, `medium`, or `high`. `medium` is the default.
          module SearchContextSize
            extend OpenAI::Internal::Type::Enum

            TaggedSymbol =
              T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize
                )
              end
            OrSymbol = T.type_alias { T.any(Symbol, String) }

            LOW =
              T.let(
                :low,
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::TaggedSymbol
              )
            MEDIUM =
              T.let(
                :medium,
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::TaggedSymbol
              )
            HIGH =
              T.let(
                :high,
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::TaggedSymbol
              )

            sig do
              override.returns(
                T::Array[
                  OpenAI::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize::TaggedSymbol
                ]
              )
            end
            def self.values
            end
          end

          class UserLocation < OpenAI::Internal::Type::BaseModel
            OrHash =
              T.type_alias do
                T.any(
                  OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation,
                  OpenAI::Internal::AnyHash
                )
              end

            # Approximate location parameters for the search.
            sig do
              returns(
                OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::Approximate
              )
            end
            attr_reader :approximate

            sig do
              params(
                approximate:
                  OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::Approximate::OrHash
              ).void
            end
            attr_writer :approximate

            # The type of location approximation. Always `approximate`.
            sig { returns(Symbol) }
            attr_accessor :type

            # Approximate location parameters for the search.
            sig do
              params(
                approximate:
                  OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::Approximate::OrHash,
                type: Symbol
              ).returns(T.attached_class)
            end
            def self.new(
              # Approximate location parameters for the search.
              approximate:,
              # The type of location approximation. Always `approximate`.
              type: :approximate
            )
            end

            sig do
              override.returns(
                {
                  approximate:
                    OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::Approximate,
                  type: Symbol
                }
              )
            end
            def to_hash
            end

            class Approximate < OpenAI::Internal::Type::BaseModel
              OrHash =
                T.type_alias do
                  T.any(
                    OpenAI::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::Approximate,
                    OpenAI::Internal::AnyHash
                  )
                end

              # Free text input for the city of the user, e.g. `San Francisco`.
              sig { returns(T.nilable(String)) }
              attr_reader :city

              sig { params(city: String).void }
              attr_writer :city

              # The two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of
              # the user, e.g. `US`.
              sig { returns(T.nilable(String)) }
              attr_reader :country

              sig { params(country: String).void }
              attr_writer :country

              # Free text input for the region of the user, e.g. `California`.
              sig { returns(T.nilable(String)) }
              attr_reader :region

              sig { params(region: String).void }
              attr_writer :region

              # The [IANA timezone](https://timeapi.io/documentation/iana-timezones) of the
              # user, e.g. `America/Los_Angeles`.
              sig { returns(T.nilable(String)) }
              attr_reader :timezone

              sig { params(timezone: String).void }
              attr_writer :timezone

              # Approximate location parameters for the search.
              sig do
                params(
                  city: String,
                  country: String,
                  region: String,
                  timezone: String
                ).returns(T.attached_class)
              end
              def self.new(
                # Free text input for the city of the user, e.g. `San Francisco`.
                city: nil,
                # The two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of
                # the user, e.g. `US`.
                country: nil,
                # Free text input for the region of the user, e.g. `California`.
                region: nil,
                # The [IANA timezone](https://timeapi.io/documentation/iana-timezones) of the
                # user, e.g. `America/Los_Angeles`.
                timezone: nil
              )
              end

              sig do
                override.returns(
                  {
                    city: String,
                    country: String,
                    region: String,
                    timezone: String
                  }
                )
              end
              def to_hash
              end
            end
          end
        end
      end
    end
  end
end
