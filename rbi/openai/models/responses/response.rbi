# typed: strong

module OpenAI
  module Models
    module Responses
      class Response < OpenAI::Internal::Type::BaseModel
        OrHash =
          T.type_alias do
            T.any(OpenAI::Responses::Response, OpenAI::Internal::AnyHash)
          end

        # Unique identifier for this Response.
        sig { returns(String) }
        attr_accessor :id

        # Unix timestamp (in seconds) of when this Response was created.
        sig { returns(Float) }
        attr_accessor :created_at

        # An error object returned when the model fails to generate a Response.
        sig { returns(T.nilable(OpenAI::Responses::ResponseError)) }
        attr_reader :error

        sig do
          params(
            error: T.nilable(OpenAI::Responses::ResponseError::OrHash)
          ).void
        end
        attr_writer :error

        # Details about why the response is incomplete.
        sig do
          returns(T.nilable(OpenAI::Responses::Response::IncompleteDetails))
        end
        attr_reader :incomplete_details

        sig do
          params(
            incomplete_details:
              T.nilable(OpenAI::Responses::Response::IncompleteDetails::OrHash)
          ).void
        end
        attr_writer :incomplete_details

        # Inserts a system (or developer) message as the first item in the model's
        # context.
        #
        # When using along with `previous_response_id`, the instructions from a previous
        # response will not be carried over to the next response. This makes it simple to
        # swap out system (or developer) messages in new responses.
        sig { returns(T.nilable(String)) }
        attr_accessor :instructions

        # Set of 16 key-value pairs that can be attached to an object. This can be useful
        # for storing additional information about the object in a structured format, and
        # querying for objects via API or the dashboard.
        #
        # Keys are strings with a maximum length of 64 characters. Values are strings with
        # a maximum length of 512 characters.
        sig { returns(T.nilable(T::Hash[Symbol, String])) }
        attr_accessor :metadata

        # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
        # wide range of models with different capabilities, performance characteristics,
        # and price points. Refer to the
        # [model guide](https://platform.openai.com/docs/models) to browse and compare
        # available models.
        sig { returns(OpenAI::ResponsesModel::Variants) }
        attr_accessor :model

        # The object type of this resource - always set to `response`.
        sig { returns(Symbol) }
        attr_accessor :object

        # An array of content items generated by the model.
        #
        # - The length and order of items in the `output` array is dependent on the
        #   model's response.
        # - Rather than accessing the first item in the `output` array and assuming it's
        #   an `assistant` message with the content generated by the model, you might
        #   consider using the `output_text` property where supported in SDKs.
        sig do
          returns(T::Array[OpenAI::Responses::ResponseOutputItem::Variants])
        end
        attr_accessor :output

        # Whether to allow the model to run tool calls in parallel.
        sig { returns(T::Boolean) }
        attr_accessor :parallel_tool_calls

        # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
        # make the output more random, while lower values like 0.2 will make it more
        # focused and deterministic. We generally recommend altering this or `top_p` but
        # not both.
        sig { returns(T.nilable(Float)) }
        attr_accessor :temperature

        # How the model should select which tool (or tools) to use when generating a
        # response. See the `tools` parameter to see how to specify which tools the model
        # can call.
        sig { returns(OpenAI::Responses::Response::ToolChoice::Variants) }
        attr_accessor :tool_choice

        # An array of tools the model may call while generating a response. You can
        # specify which tool to use by setting the `tool_choice` parameter.
        #
        # The two categories of tools you can provide the model are:
        #
        # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
        #   capabilities, like
        #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
        #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
        #   Learn more about
        #   [built-in tools](https://platform.openai.com/docs/guides/tools).
        # - **Function calls (custom tools)**: Functions that are defined by you, enabling
        #   the model to call your own code. Learn more about
        #   [function calling](https://platform.openai.com/docs/guides/function-calling).
        sig { returns(T::Array[OpenAI::Responses::Tool::Variants]) }
        attr_accessor :tools

        # An alternative to sampling with temperature, called nucleus sampling, where the
        # model considers the results of the tokens with top_p probability mass. So 0.1
        # means only the tokens comprising the top 10% probability mass are considered.
        #
        # We generally recommend altering this or `temperature` but not both.
        sig { returns(T.nilable(Float)) }
        attr_accessor :top_p

        # Whether to run the model response in the background.
        # [Learn more](https://platform.openai.com/docs/guides/background).
        sig { returns(T.nilable(T::Boolean)) }
        attr_accessor :background

        # An upper bound for the number of tokens that can be generated for a response,
        # including visible output tokens and
        # [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
        sig { returns(T.nilable(Integer)) }
        attr_accessor :max_output_tokens

        # The unique ID of the previous response to the model. Use this to create
        # multi-turn conversations. Learn more about
        # [conversation state](https://platform.openai.com/docs/guides/conversation-state).
        sig { returns(T.nilable(String)) }
        attr_accessor :previous_response_id

        # **o-series models only**
        #
        # Configuration options for
        # [reasoning models](https://platform.openai.com/docs/guides/reasoning).
        sig { returns(T.nilable(OpenAI::Reasoning)) }
        attr_reader :reasoning

        sig { params(reasoning: T.nilable(OpenAI::Reasoning::OrHash)).void }
        attr_writer :reasoning

        # Specifies the latency tier to use for processing the request. This parameter is
        # relevant for customers subscribed to the scale tier service:
        #
        # - If set to 'auto', and the Project is Scale tier enabled, the system will
        #   utilize scale tier credits until they are exhausted.
        # - If set to 'auto', and the Project is not Scale tier enabled, the request will
        #   be processed using the default service tier with a lower uptime SLA and no
        #   latency guarantee.
        # - If set to 'default', the request will be processed using the default service
        #   tier with a lower uptime SLA and no latency guarantee.
        # - If set to 'flex', the request will be processed with the Flex Processing
        #   service tier.
        #   [Learn more](https://platform.openai.com/docs/guides/flex-processing).
        # - When not set, the default behavior is 'auto'.
        #
        # When this parameter is set, the response body will include the `service_tier`
        # utilized.
        sig do
          returns(
            T.nilable(OpenAI::Responses::Response::ServiceTier::TaggedSymbol)
          )
        end
        attr_accessor :service_tier

        # The status of the response generation. One of `completed`, `failed`,
        # `in_progress`, `cancelled`, `queued`, or `incomplete`.
        sig do
          returns(T.nilable(OpenAI::Responses::ResponseStatus::TaggedSymbol))
        end
        attr_reader :status

        sig { params(status: OpenAI::Responses::ResponseStatus::OrSymbol).void }
        attr_writer :status

        # Configuration options for a text response from the model. Can be plain text or
        # structured JSON data. Learn more:
        #
        # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
        # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
        sig { returns(T.nilable(OpenAI::Responses::ResponseTextConfig)) }
        attr_reader :text

        sig { params(text: OpenAI::Responses::ResponseTextConfig::OrHash).void }
        attr_writer :text

        # The truncation strategy to use for the model response.
        #
        # - `auto`: If the context of this response and previous ones exceeds the model's
        #   context window size, the model will truncate the response to fit the context
        #   window by dropping input items in the middle of the conversation.
        # - `disabled` (default): If a model response will exceed the context window size
        #   for a model, the request will fail with a 400 error.
        sig do
          returns(
            T.nilable(OpenAI::Responses::Response::Truncation::TaggedSymbol)
          )
        end
        attr_accessor :truncation

        # Represents token usage details including input tokens, output tokens, a
        # breakdown of output tokens, and the total tokens used.
        sig { returns(T.nilable(OpenAI::Responses::ResponseUsage)) }
        attr_reader :usage

        sig { params(usage: OpenAI::Responses::ResponseUsage::OrHash).void }
        attr_writer :usage

        # A stable identifier for your end-users. Used to boost cache hit rates by better
        # bucketing similar requests and to help OpenAI detect and prevent abuse.
        # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
        sig { returns(T.nilable(String)) }
        attr_reader :user

        sig { params(user: String).void }
        attr_writer :user

        sig do
          params(
            id: String,
            created_at: Float,
            error: T.nilable(OpenAI::Responses::ResponseError::OrHash),
            incomplete_details:
              T.nilable(OpenAI::Responses::Response::IncompleteDetails::OrHash),
            instructions: T.nilable(String),
            metadata: T.nilable(T::Hash[Symbol, String]),
            model:
              T.any(
                String,
                OpenAI::ChatModel::OrSymbol,
                OpenAI::ResponsesModel::ResponsesOnlyModel::OrSymbol
              ),
            output:
              T::Array[
                T.any(
                  OpenAI::Responses::ResponseOutputMessage::OrHash,
                  OpenAI::Responses::ResponseFileSearchToolCall::OrHash,
                  OpenAI::Responses::ResponseFunctionToolCall::OrHash,
                  OpenAI::Responses::ResponseFunctionWebSearch::OrHash,
                  OpenAI::Responses::ResponseComputerToolCall::OrHash,
                  OpenAI::Responses::ResponseReasoningItem::OrHash,
                  OpenAI::Responses::ResponseOutputItem::ImageGenerationCall::OrHash,
                  OpenAI::Responses::ResponseCodeInterpreterToolCall::OrHash,
                  OpenAI::Responses::ResponseOutputItem::LocalShellCall::OrHash,
                  OpenAI::Responses::ResponseOutputItem::McpCall::OrHash,
                  OpenAI::Responses::ResponseOutputItem::McpListTools::OrHash,
                  OpenAI::Responses::ResponseOutputItem::McpApprovalRequest::OrHash
                )
              ],
            parallel_tool_calls: T::Boolean,
            temperature: T.nilable(Float),
            tool_choice:
              T.any(
                OpenAI::Responses::ToolChoiceOptions::OrSymbol,
                OpenAI::Responses::ToolChoiceTypes::OrHash,
                OpenAI::Responses::ToolChoiceFunction::OrHash
              ),
            tools:
              T::Array[
                T.any(
                  OpenAI::Responses::FunctionTool::OrHash,
                  OpenAI::Responses::FileSearchTool::OrHash,
                  OpenAI::Responses::ComputerTool::OrHash,
                  OpenAI::Responses::Tool::Mcp::OrHash,
                  OpenAI::Responses::Tool::CodeInterpreter::OrHash,
                  OpenAI::Responses::Tool::ImageGeneration::OrHash,
                  OpenAI::Responses::Tool::LocalShell::OrHash,
                  OpenAI::Responses::WebSearchTool::OrHash
                )
              ],
            top_p: T.nilable(Float),
            background: T.nilable(T::Boolean),
            max_output_tokens: T.nilable(Integer),
            previous_response_id: T.nilable(String),
            reasoning: T.nilable(OpenAI::Reasoning::OrHash),
            service_tier:
              T.nilable(OpenAI::Responses::Response::ServiceTier::OrSymbol),
            status: OpenAI::Responses::ResponseStatus::OrSymbol,
            text: OpenAI::Responses::ResponseTextConfig::OrHash,
            truncation:
              T.nilable(OpenAI::Responses::Response::Truncation::OrSymbol),
            usage: OpenAI::Responses::ResponseUsage::OrHash,
            user: String,
            object: Symbol
          ).returns(T.attached_class)
        end
        def self.new(
          # Unique identifier for this Response.
          id:,
          # Unix timestamp (in seconds) of when this Response was created.
          created_at:,
          # An error object returned when the model fails to generate a Response.
          error:,
          # Details about why the response is incomplete.
          incomplete_details:,
          # Inserts a system (or developer) message as the first item in the model's
          # context.
          #
          # When using along with `previous_response_id`, the instructions from a previous
          # response will not be carried over to the next response. This makes it simple to
          # swap out system (or developer) messages in new responses.
          instructions:,
          # Set of 16 key-value pairs that can be attached to an object. This can be useful
          # for storing additional information about the object in a structured format, and
          # querying for objects via API or the dashboard.
          #
          # Keys are strings with a maximum length of 64 characters. Values are strings with
          # a maximum length of 512 characters.
          metadata:,
          # Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
          # wide range of models with different capabilities, performance characteristics,
          # and price points. Refer to the
          # [model guide](https://platform.openai.com/docs/models) to browse and compare
          # available models.
          model:,
          # An array of content items generated by the model.
          #
          # - The length and order of items in the `output` array is dependent on the
          #   model's response.
          # - Rather than accessing the first item in the `output` array and assuming it's
          #   an `assistant` message with the content generated by the model, you might
          #   consider using the `output_text` property where supported in SDKs.
          output:,
          # Whether to allow the model to run tool calls in parallel.
          parallel_tool_calls:,
          # What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
          # make the output more random, while lower values like 0.2 will make it more
          # focused and deterministic. We generally recommend altering this or `top_p` but
          # not both.
          temperature:,
          # How the model should select which tool (or tools) to use when generating a
          # response. See the `tools` parameter to see how to specify which tools the model
          # can call.
          tool_choice:,
          # An array of tools the model may call while generating a response. You can
          # specify which tool to use by setting the `tool_choice` parameter.
          #
          # The two categories of tools you can provide the model are:
          #
          # - **Built-in tools**: Tools that are provided by OpenAI that extend the model's
          #   capabilities, like
          #   [web search](https://platform.openai.com/docs/guides/tools-web-search) or
          #   [file search](https://platform.openai.com/docs/guides/tools-file-search).
          #   Learn more about
          #   [built-in tools](https://platform.openai.com/docs/guides/tools).
          # - **Function calls (custom tools)**: Functions that are defined by you, enabling
          #   the model to call your own code. Learn more about
          #   [function calling](https://platform.openai.com/docs/guides/function-calling).
          tools:,
          # An alternative to sampling with temperature, called nucleus sampling, where the
          # model considers the results of the tokens with top_p probability mass. So 0.1
          # means only the tokens comprising the top 10% probability mass are considered.
          #
          # We generally recommend altering this or `temperature` but not both.
          top_p:,
          # Whether to run the model response in the background.
          # [Learn more](https://platform.openai.com/docs/guides/background).
          background: nil,
          # An upper bound for the number of tokens that can be generated for a response,
          # including visible output tokens and
          # [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
          max_output_tokens: nil,
          # The unique ID of the previous response to the model. Use this to create
          # multi-turn conversations. Learn more about
          # [conversation state](https://platform.openai.com/docs/guides/conversation-state).
          previous_response_id: nil,
          # **o-series models only**
          #
          # Configuration options for
          # [reasoning models](https://platform.openai.com/docs/guides/reasoning).
          reasoning: nil,
          # Specifies the latency tier to use for processing the request. This parameter is
          # relevant for customers subscribed to the scale tier service:
          #
          # - If set to 'auto', and the Project is Scale tier enabled, the system will
          #   utilize scale tier credits until they are exhausted.
          # - If set to 'auto', and the Project is not Scale tier enabled, the request will
          #   be processed using the default service tier with a lower uptime SLA and no
          #   latency guarantee.
          # - If set to 'default', the request will be processed using the default service
          #   tier with a lower uptime SLA and no latency guarantee.
          # - If set to 'flex', the request will be processed with the Flex Processing
          #   service tier.
          #   [Learn more](https://platform.openai.com/docs/guides/flex-processing).
          # - When not set, the default behavior is 'auto'.
          #
          # When this parameter is set, the response body will include the `service_tier`
          # utilized.
          service_tier: nil,
          # The status of the response generation. One of `completed`, `failed`,
          # `in_progress`, `cancelled`, `queued`, or `incomplete`.
          status: nil,
          # Configuration options for a text response from the model. Can be plain text or
          # structured JSON data. Learn more:
          #
          # - [Text inputs and outputs](https://platform.openai.com/docs/guides/text)
          # - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
          text: nil,
          # The truncation strategy to use for the model response.
          #
          # - `auto`: If the context of this response and previous ones exceeds the model's
          #   context window size, the model will truncate the response to fit the context
          #   window by dropping input items in the middle of the conversation.
          # - `disabled` (default): If a model response will exceed the context window size
          #   for a model, the request will fail with a 400 error.
          truncation: nil,
          # Represents token usage details including input tokens, output tokens, a
          # breakdown of output tokens, and the total tokens used.
          usage: nil,
          # A stable identifier for your end-users. Used to boost cache hit rates by better
          # bucketing similar requests and to help OpenAI detect and prevent abuse.
          # [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
          user: nil,
          # The object type of this resource - always set to `response`.
          object: :response
        )
        end

        sig do
          override.returns(
            {
              id: String,
              created_at: Float,
              error: T.nilable(OpenAI::Responses::ResponseError),
              incomplete_details:
                T.nilable(OpenAI::Responses::Response::IncompleteDetails),
              instructions: T.nilable(String),
              metadata: T.nilable(T::Hash[Symbol, String]),
              model: OpenAI::ResponsesModel::Variants,
              object: Symbol,
              output: T::Array[OpenAI::Responses::ResponseOutputItem::Variants],
              parallel_tool_calls: T::Boolean,
              temperature: T.nilable(Float),
              tool_choice: OpenAI::Responses::Response::ToolChoice::Variants,
              tools: T::Array[OpenAI::Responses::Tool::Variants],
              top_p: T.nilable(Float),
              background: T.nilable(T::Boolean),
              max_output_tokens: T.nilable(Integer),
              previous_response_id: T.nilable(String),
              reasoning: T.nilable(OpenAI::Reasoning),
              service_tier:
                T.nilable(
                  OpenAI::Responses::Response::ServiceTier::TaggedSymbol
                ),
              status: OpenAI::Responses::ResponseStatus::TaggedSymbol,
              text: OpenAI::Responses::ResponseTextConfig,
              truncation:
                T.nilable(
                  OpenAI::Responses::Response::Truncation::TaggedSymbol
                ),
              usage: OpenAI::Responses::ResponseUsage,
              user: String
            }
          )
        end
        def to_hash
        end

        class IncompleteDetails < OpenAI::Internal::Type::BaseModel
          OrHash =
            T.type_alias do
              T.any(
                OpenAI::Responses::Response::IncompleteDetails,
                OpenAI::Internal::AnyHash
              )
            end

          # The reason why the response is incomplete.
          sig do
            returns(
              T.nilable(
                OpenAI::Responses::Response::IncompleteDetails::Reason::TaggedSymbol
              )
            )
          end
          attr_reader :reason

          sig do
            params(
              reason:
                OpenAI::Responses::Response::IncompleteDetails::Reason::OrSymbol
            ).void
          end
          attr_writer :reason

          # Details about why the response is incomplete.
          sig do
            params(
              reason:
                OpenAI::Responses::Response::IncompleteDetails::Reason::OrSymbol
            ).returns(T.attached_class)
          end
          def self.new(
            # The reason why the response is incomplete.
            reason: nil
          )
          end

          sig do
            override.returns(
              {
                reason:
                  OpenAI::Responses::Response::IncompleteDetails::Reason::TaggedSymbol
              }
            )
          end
          def to_hash
          end

          # The reason why the response is incomplete.
          module Reason
            extend OpenAI::Internal::Type::Enum

            TaggedSymbol =
              T.type_alias do
                T.all(
                  Symbol,
                  OpenAI::Responses::Response::IncompleteDetails::Reason
                )
              end
            OrSymbol = T.type_alias { T.any(Symbol, String) }

            MAX_OUTPUT_TOKENS =
              T.let(
                :max_output_tokens,
                OpenAI::Responses::Response::IncompleteDetails::Reason::TaggedSymbol
              )
            CONTENT_FILTER =
              T.let(
                :content_filter,
                OpenAI::Responses::Response::IncompleteDetails::Reason::TaggedSymbol
              )

            sig do
              override.returns(
                T::Array[
                  OpenAI::Responses::Response::IncompleteDetails::Reason::TaggedSymbol
                ]
              )
            end
            def self.values
            end
          end
        end

        # How the model should select which tool (or tools) to use when generating a
        # response. See the `tools` parameter to see how to specify which tools the model
        # can call.
        module ToolChoice
          extend OpenAI::Internal::Type::Union

          Variants =
            T.type_alias do
              T.any(
                OpenAI::Responses::ToolChoiceOptions::TaggedSymbol,
                OpenAI::Responses::ToolChoiceTypes,
                OpenAI::Responses::ToolChoiceFunction
              )
            end

          sig do
            override.returns(
              T::Array[OpenAI::Responses::Response::ToolChoice::Variants]
            )
          end
          def self.variants
          end
        end

        # Specifies the latency tier to use for processing the request. This parameter is
        # relevant for customers subscribed to the scale tier service:
        #
        # - If set to 'auto', and the Project is Scale tier enabled, the system will
        #   utilize scale tier credits until they are exhausted.
        # - If set to 'auto', and the Project is not Scale tier enabled, the request will
        #   be processed using the default service tier with a lower uptime SLA and no
        #   latency guarantee.
        # - If set to 'default', the request will be processed using the default service
        #   tier with a lower uptime SLA and no latency guarantee.
        # - If set to 'flex', the request will be processed with the Flex Processing
        #   service tier.
        #   [Learn more](https://platform.openai.com/docs/guides/flex-processing).
        # - When not set, the default behavior is 'auto'.
        #
        # When this parameter is set, the response body will include the `service_tier`
        # utilized.
        module ServiceTier
          extend OpenAI::Internal::Type::Enum

          TaggedSymbol =
            T.type_alias do
              T.all(Symbol, OpenAI::Responses::Response::ServiceTier)
            end
          OrSymbol = T.type_alias { T.any(Symbol, String) }

          AUTO =
            T.let(:auto, OpenAI::Responses::Response::ServiceTier::TaggedSymbol)
          DEFAULT =
            T.let(
              :default,
              OpenAI::Responses::Response::ServiceTier::TaggedSymbol
            )
          FLEX =
            T.let(:flex, OpenAI::Responses::Response::ServiceTier::TaggedSymbol)

          sig do
            override.returns(
              T::Array[OpenAI::Responses::Response::ServiceTier::TaggedSymbol]
            )
          end
          def self.values
          end
        end

        # The truncation strategy to use for the model response.
        #
        # - `auto`: If the context of this response and previous ones exceeds the model's
        #   context window size, the model will truncate the response to fit the context
        #   window by dropping input items in the middle of the conversation.
        # - `disabled` (default): If a model response will exceed the context window size
        #   for a model, the request will fail with a 400 error.
        module Truncation
          extend OpenAI::Internal::Type::Enum

          TaggedSymbol =
            T.type_alias do
              T.all(Symbol, OpenAI::Responses::Response::Truncation)
            end
          OrSymbol = T.type_alias { T.any(Symbol, String) }

          AUTO =
            T.let(:auto, OpenAI::Responses::Response::Truncation::TaggedSymbol)
          DISABLED =
            T.let(
              :disabled,
              OpenAI::Responses::Response::Truncation::TaggedSymbol
            )

          sig do
            override.returns(
              T::Array[OpenAI::Responses::Response::Truncation::TaggedSymbol]
            )
          end
          def self.values
          end
        end
      end
    end
  end
end
