# frozen_string_literal: true

module OpenAI
  module Models
    class CompletionUsage < OpenAI::Internal::Type::BaseModel
      # @!attribute completion_tokens
      #   Number of tokens in the generated completion.
      #
      #   @return [Integer]
      required :completion_tokens, Integer

      # @!attribute prompt_tokens
      #   Number of tokens in the prompt.
      #
      #   @return [Integer]
      required :prompt_tokens, Integer

      # @!attribute total_tokens
      #   Total number of tokens used in the request (prompt + completion).
      #
      #   @return [Integer]
      required :total_tokens, Integer

      # @!attribute completion_tokens_details
      #   Breakdown of tokens used in a completion.
      #
      #   @return [OpenAI::Models::CompletionUsage::CompletionTokensDetails, nil]
      optional :completion_tokens_details, -> { OpenAI::Models::CompletionUsage::CompletionTokensDetails }

      # @!attribute prompt_tokens_details
      #   Breakdown of tokens used in the prompt.
      #
      #   @return [OpenAI::Models::CompletionUsage::PromptTokensDetails, nil]
      optional :prompt_tokens_details, -> { OpenAI::Models::CompletionUsage::PromptTokensDetails }

      # @!method initialize(completion_tokens:, prompt_tokens:, total_tokens:, completion_tokens_details: nil, prompt_tokens_details: nil)
      #   Usage statistics for the completion request.
      #
      #   @param completion_tokens [Integer]
      #   @param prompt_tokens [Integer]
      #   @param total_tokens [Integer]
      #   @param completion_tokens_details [OpenAI::Models::CompletionUsage::CompletionTokensDetails]
      #   @param prompt_tokens_details [OpenAI::Models::CompletionUsage::PromptTokensDetails]

      # @see OpenAI::Models::CompletionUsage#completion_tokens_details
      class CompletionTokensDetails < OpenAI::Internal::Type::BaseModel
        # @!attribute accepted_prediction_tokens
        #   When using Predicted Outputs, the number of tokens in the prediction that
        #   appeared in the completion.
        #
        #   @return [Integer, nil]
        optional :accepted_prediction_tokens, Integer

        # @!attribute audio_tokens
        #   Audio input tokens generated by the model.
        #
        #   @return [Integer, nil]
        optional :audio_tokens, Integer

        # @!attribute reasoning_tokens
        #   Tokens generated by the model for reasoning.
        #
        #   @return [Integer, nil]
        optional :reasoning_tokens, Integer

        # @!attribute rejected_prediction_tokens
        #   When using Predicted Outputs, the number of tokens in the prediction that did
        #   not appear in the completion. However, like reasoning tokens, these tokens are
        #   still counted in the total completion tokens for purposes of billing, output,
        #   and context window limits.
        #
        #   @return [Integer, nil]
        optional :rejected_prediction_tokens, Integer

        # @!method initialize(accepted_prediction_tokens: nil, audio_tokens: nil, reasoning_tokens: nil, rejected_prediction_tokens: nil)
        #   Breakdown of tokens used in a completion.
        #
        #   @param accepted_prediction_tokens [Integer]
        #   @param audio_tokens [Integer]
        #   @param reasoning_tokens [Integer]
        #   @param rejected_prediction_tokens [Integer]
      end

      # @see OpenAI::Models::CompletionUsage#prompt_tokens_details
      class PromptTokensDetails < OpenAI::Internal::Type::BaseModel
        # @!attribute audio_tokens
        #   Audio input tokens present in the prompt.
        #
        #   @return [Integer, nil]
        optional :audio_tokens, Integer

        # @!attribute cached_tokens
        #   Cached tokens present in the prompt.
        #
        #   @return [Integer, nil]
        optional :cached_tokens, Integer

        # @!method initialize(audio_tokens: nil, cached_tokens: nil)
        #   Breakdown of tokens used in the prompt.
        #
        #   @param audio_tokens [Integer]
        #   @param cached_tokens [Integer]
      end
    end
  end
end
