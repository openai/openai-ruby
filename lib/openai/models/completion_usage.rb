# frozen_string_literal: true

module OpenAI
  module Models
    class CompletionUsage < OpenAI::Internal::Type::BaseModel
      # @!attribute completion_tokens
      #   Number of tokens in the generated completion.
      #
      #   @return [Integer]
      required :completion_tokens, Integer

      # @!attribute prompt_tokens
      #   Number of tokens in the prompt.
      #
      #   @return [Integer]
      required :prompt_tokens, Integer

      # @!attribute total_tokens
      #   Total number of tokens used in the request (prompt + completion).
      #
      #   @return [Integer]
      required :total_tokens, Integer

      # @!attribute [r] completion_tokens_details
      #   Breakdown of tokens used in a completion.
      #
      #   @return [OpenAI::Models::CompletionUsage::CompletionTokensDetails, nil]
      optional :completion_tokens_details, -> { OpenAI::Models::CompletionUsage::CompletionTokensDetails }

      # @!parse
      #   # @return [OpenAI::Models::CompletionUsage::CompletionTokensDetails]
      #   attr_writer :completion_tokens_details

      # @!attribute [r] prompt_tokens_details
      #   Breakdown of tokens used in the prompt.
      #
      #   @return [OpenAI::Models::CompletionUsage::PromptTokensDetails, nil]
      optional :prompt_tokens_details, -> { OpenAI::Models::CompletionUsage::PromptTokensDetails }

      # @!parse
      #   # @return [OpenAI::Models::CompletionUsage::PromptTokensDetails]
      #   attr_writer :prompt_tokens_details

      # @!parse
      #   # Usage statistics for the completion request.
      #   #
      #   # @param completion_tokens [Integer]
      #   # @param prompt_tokens [Integer]
      #   # @param total_tokens [Integer]
      #   # @param completion_tokens_details [OpenAI::Models::CompletionUsage::CompletionTokensDetails]
      #   # @param prompt_tokens_details [OpenAI::Models::CompletionUsage::PromptTokensDetails]
      #   #
      #   def initialize(
      #     completion_tokens:,
      #     prompt_tokens:,
      #     total_tokens:,
      #     completion_tokens_details: nil,
      #     prompt_tokens_details: nil,
      #     **
      #   )
      #     super
      #   end

      # def initialize: (Hash | OpenAI::Internal::Type::BaseModel) -> void

      # @see OpenAI::Models::CompletionUsage#completion_tokens_details
      class CompletionTokensDetails < OpenAI::Internal::Type::BaseModel
        # @!attribute [r] accepted_prediction_tokens
        #   When using Predicted Outputs, the number of tokens in the prediction that
        #   appeared in the completion.
        #
        #   @return [Integer, nil]
        optional :accepted_prediction_tokens, Integer

        # @!parse
        #   # @return [Integer]
        #   attr_writer :accepted_prediction_tokens

        # @!attribute [r] audio_tokens
        #   Audio input tokens generated by the model.
        #
        #   @return [Integer, nil]
        optional :audio_tokens, Integer

        # @!parse
        #   # @return [Integer]
        #   attr_writer :audio_tokens

        # @!attribute [r] reasoning_tokens
        #   Tokens generated by the model for reasoning.
        #
        #   @return [Integer, nil]
        optional :reasoning_tokens, Integer

        # @!parse
        #   # @return [Integer]
        #   attr_writer :reasoning_tokens

        # @!attribute [r] rejected_prediction_tokens
        #   When using Predicted Outputs, the number of tokens in the prediction that did
        #   not appear in the completion. However, like reasoning tokens, these tokens are
        #   still counted in the total completion tokens for purposes of billing, output,
        #   and context window limits.
        #
        #   @return [Integer, nil]
        optional :rejected_prediction_tokens, Integer

        # @!parse
        #   # @return [Integer]
        #   attr_writer :rejected_prediction_tokens

        # @!parse
        #   # Breakdown of tokens used in a completion.
        #   #
        #   # @param accepted_prediction_tokens [Integer]
        #   # @param audio_tokens [Integer]
        #   # @param reasoning_tokens [Integer]
        #   # @param rejected_prediction_tokens [Integer]
        #   #
        #   def initialize(
        #     accepted_prediction_tokens: nil,
        #     audio_tokens: nil,
        #     reasoning_tokens: nil,
        #     rejected_prediction_tokens: nil,
        #     **
        #   )
        #     super
        #   end

        # def initialize: (Hash | OpenAI::Internal::Type::BaseModel) -> void
      end

      # @see OpenAI::Models::CompletionUsage#prompt_tokens_details
      class PromptTokensDetails < OpenAI::Internal::Type::BaseModel
        # @!attribute [r] audio_tokens
        #   Audio input tokens present in the prompt.
        #
        #   @return [Integer, nil]
        optional :audio_tokens, Integer

        # @!parse
        #   # @return [Integer]
        #   attr_writer :audio_tokens

        # @!attribute [r] cached_tokens
        #   Cached tokens present in the prompt.
        #
        #   @return [Integer, nil]
        optional :cached_tokens, Integer

        # @!parse
        #   # @return [Integer]
        #   attr_writer :cached_tokens

        # @!parse
        #   # Breakdown of tokens used in the prompt.
        #   #
        #   # @param audio_tokens [Integer]
        #   # @param cached_tokens [Integer]
        #   #
        #   def initialize(audio_tokens: nil, cached_tokens: nil, **) = super

        # def initialize: (Hash | OpenAI::Internal::Type::BaseModel) -> void
      end
    end
  end
end
