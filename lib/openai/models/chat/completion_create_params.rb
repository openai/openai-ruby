# frozen_string_literal: true

module OpenAI
  module Models
    module Chat
      class CompletionCreateParams < OpenAI::BaseModel
        # @!parse
        #   extend OpenAI::RequestParameters::Converter
        include OpenAI::RequestParameters

        # @!attribute messages
        #   A list of messages comprising the conversation so far. Depending on the
        #     [model](https://platform.openai.com/docs/models) you use, different message
        #     types (modalities) are supported, like
        #     [text](https://platform.openai.com/docs/guides/text-generation),
        #     [images](https://platform.openai.com/docs/guides/vision), and
        #     [audio](https://platform.openai.com/docs/guides/audio).
        #
        #   @return [Array<OpenAI::Models::Chat::ChatCompletionDeveloperMessageParam, OpenAI::Models::Chat::ChatCompletionSystemMessageParam, OpenAI::Models::Chat::ChatCompletionUserMessageParam, OpenAI::Models::Chat::ChatCompletionAssistantMessageParam, OpenAI::Models::Chat::ChatCompletionToolMessageParam, OpenAI::Models::Chat::ChatCompletionFunctionMessageParam>]
        required :messages, -> { OpenAI::ArrayOf[union: OpenAI::Models::Chat::ChatCompletionMessageParam] }

        # @!attribute model
        #   Model ID used to generate the response, like `gpt-4o` or `o1`. OpenAI offers a
        #     wide range of models with different capabilities, performance characteristics,
        #     and price points. Refer to the
        #     [model guide](https://platform.openai.com/docs/models) to browse and compare
        #     available models.
        #
        #   @return [String, Symbol, OpenAI::Models::ChatModel]
        required :model, union: -> { OpenAI::Models::Chat::CompletionCreateParams::Model }

        # @!attribute audio
        #   Parameters for audio output. Required when audio output is requested with
        #     `modalities: ["audio"]`.
        #     [Learn more](https://platform.openai.com/docs/guides/audio).
        #
        #   @return [OpenAI::Models::Chat::ChatCompletionAudioParam, nil]
        optional :audio, -> { OpenAI::Models::Chat::ChatCompletionAudioParam }, nil?: true

        # @!attribute frequency_penalty
        #   Number between -2.0 and 2.0. Positive values penalize new tokens based on their
        #     existing frequency in the text so far, decreasing the model's likelihood to
        #     repeat the same line verbatim.
        #
        #   @return [Float, nil]
        optional :frequency_penalty, Float, nil?: true

        # @!attribute [r] function_call
        #   Deprecated in favor of `tool_choice`.
        #
        #     Controls which (if any) function is called by the model.
        #
        #     `none` means the model will not call a function and instead generates a message.
        #
        #     `auto` means the model can pick between generating a message or calling a
        #     function.
        #
        #     Specifying a particular function via `{"name": "my_function"}` forces the model
        #     to call that function.
        #
        #     `none` is the default when no functions are present. `auto` is the default if
        #     functions are present.
        #
        #   @return [Symbol, OpenAI::Models::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode, OpenAI::Models::Chat::ChatCompletionFunctionCallOption, nil]
        optional :function_call, union: -> { OpenAI::Models::Chat::CompletionCreateParams::FunctionCall }

        # @!parse
        #   # @return [Symbol, OpenAI::Models::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode, OpenAI::Models::Chat::ChatCompletionFunctionCallOption]
        #   attr_writer :function_call

        # @!attribute [r] functions
        #   Deprecated in favor of `tools`.
        #
        #     A list of functions the model may generate JSON inputs for.
        #
        #   @return [Array<OpenAI::Models::Chat::CompletionCreateParams::Function>, nil]
        optional :functions, -> { OpenAI::ArrayOf[OpenAI::Models::Chat::CompletionCreateParams::Function] }

        # @!parse
        #   # @return [Array<OpenAI::Models::Chat::CompletionCreateParams::Function>]
        #   attr_writer :functions

        # @!attribute logit_bias
        #   Modify the likelihood of specified tokens appearing in the completion.
        #
        #     Accepts a JSON object that maps tokens (specified by their token ID in the
        #     tokenizer) to an associated bias value from -100 to 100. Mathematically, the
        #     bias is added to the logits generated by the model prior to sampling. The exact
        #     effect will vary per model, but values between -1 and 1 should decrease or
        #     increase likelihood of selection; values like -100 or 100 should result in a ban
        #     or exclusive selection of the relevant token.
        #
        #   @return [Hash{Symbol=>Integer}, nil]
        optional :logit_bias, OpenAI::HashOf[Integer], nil?: true

        # @!attribute logprobs
        #   Whether to return log probabilities of the output tokens or not. If true,
        #     returns the log probabilities of each output token returned in the `content` of
        #     `message`.
        #
        #   @return [Boolean, nil]
        optional :logprobs, OpenAI::BooleanModel, nil?: true

        # @!attribute max_completion_tokens
        #   An upper bound for the number of tokens that can be generated for a completion,
        #     including visible output tokens and
        #     [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
        #
        #   @return [Integer, nil]
        optional :max_completion_tokens, Integer, nil?: true

        # @!attribute max_tokens
        #   The maximum number of [tokens](/tokenizer) that can be generated in the chat
        #     completion. This value can be used to control
        #     [costs](https://openai.com/api/pricing/) for text generated via API.
        #
        #     This value is now deprecated in favor of `max_completion_tokens`, and is not
        #     compatible with
        #     [o1 series models](https://platform.openai.com/docs/guides/reasoning).
        #
        #   @return [Integer, nil]
        optional :max_tokens, Integer, nil?: true

        # @!attribute metadata
        #   Set of 16 key-value pairs that can be attached to an object. This can be useful
        #     for storing additional information about the object in a structured format, and
        #     querying for objects via API or the dashboard.
        #
        #     Keys are strings with a maximum length of 64 characters. Values are strings with
        #     a maximum length of 512 characters.
        #
        #   @return [Hash{Symbol=>String}, nil]
        optional :metadata, OpenAI::HashOf[String], nil?: true

        # @!attribute modalities
        #   Output types that you would like the model to generate. Most models are capable
        #     of generating text, which is the default:
        #
        #     `["text"]`
        #
        #     The `gpt-4o-audio-preview` model can also be used to
        #     [generate audio](https://platform.openai.com/docs/guides/audio). To request that
        #     this model generate both text and audio responses, you can use:
        #
        #     `["text", "audio"]`
        #
        #   @return [Array<Symbol, OpenAI::Models::Chat::CompletionCreateParams::Modality>, nil]
        optional :modalities,
                 -> { OpenAI::ArrayOf[enum: OpenAI::Models::Chat::CompletionCreateParams::Modality] },
                 nil?: true

        # @!attribute n
        #   How many chat completion choices to generate for each input message. Note that
        #     you will be charged based on the number of generated tokens across all of the
        #     choices. Keep `n` as `1` to minimize costs.
        #
        #   @return [Integer, nil]
        optional :n, Integer, nil?: true

        # @!attribute [r] parallel_tool_calls
        #   Whether to enable
        #     [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
        #     during tool use.
        #
        #   @return [Boolean, nil]
        optional :parallel_tool_calls, OpenAI::BooleanModel

        # @!parse
        #   # @return [Boolean]
        #   attr_writer :parallel_tool_calls

        # @!attribute prediction
        #   Static predicted output content, such as the content of a text file that is
        #     being regenerated.
        #
        #   @return [OpenAI::Models::Chat::ChatCompletionPredictionContent, nil]
        optional :prediction, -> { OpenAI::Models::Chat::ChatCompletionPredictionContent }, nil?: true

        # @!attribute presence_penalty
        #   Number between -2.0 and 2.0. Positive values penalize new tokens based on
        #     whether they appear in the text so far, increasing the model's likelihood to
        #     talk about new topics.
        #
        #   @return [Float, nil]
        optional :presence_penalty, Float, nil?: true

        # @!attribute reasoning_effort
        #   **o-series models only**
        #
        #     Constrains effort on reasoning for
        #     [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
        #     supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
        #     result in faster responses and fewer tokens used on reasoning in a response.
        #
        #   @return [Symbol, OpenAI::Models::ReasoningEffort, nil]
        optional :reasoning_effort, enum: -> { OpenAI::Models::ReasoningEffort }, nil?: true

        # @!attribute [r] response_format
        #   An object specifying the format that the model must output.
        #
        #     Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
        #     Outputs which ensures the model will match your supplied JSON schema. Learn more
        #     in the
        #     [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
        #
        #     Setting to `{ "type": "json_object" }` enables the older JSON mode, which
        #     ensures the message the model generates is valid JSON. Using `json_schema` is
        #     preferred for models that support it.
        #
        #   @return [OpenAI::Models::ResponseFormatText, OpenAI::Models::ResponseFormatJSONSchema, OpenAI::Models::ResponseFormatJSONObject, nil]
        optional :response_format, union: -> { OpenAI::Models::Chat::CompletionCreateParams::ResponseFormat }

        # @!parse
        #   # @return [OpenAI::Models::ResponseFormatText, OpenAI::Models::ResponseFormatJSONSchema, OpenAI::Models::ResponseFormatJSONObject]
        #   attr_writer :response_format

        # @!attribute seed
        #   This feature is in Beta. If specified, our system will make a best effort to
        #     sample deterministically, such that repeated requests with the same `seed` and
        #     parameters should return the same result. Determinism is not guaranteed, and you
        #     should refer to the `system_fingerprint` response parameter to monitor changes
        #     in the backend.
        #
        #   @return [Integer, nil]
        optional :seed, Integer, nil?: true

        # @!attribute service_tier
        #   Specifies the latency tier to use for processing the request. This parameter is
        #     relevant for customers subscribed to the scale tier service:
        #
        #     - If set to 'auto', and the Project is Scale tier enabled, the system will
        #       utilize scale tier credits until they are exhausted.
        #     - If set to 'auto', and the Project is not Scale tier enabled, the request will
        #       be processed using the default service tier with a lower uptime SLA and no
        #       latency guarentee.
        #     - If set to 'default', the request will be processed using the default service
        #       tier with a lower uptime SLA and no latency guarentee.
        #     - When not set, the default behavior is 'auto'.
        #
        #     When this parameter is set, the response body will include the `service_tier`
        #     utilized.
        #
        #   @return [Symbol, OpenAI::Models::Chat::CompletionCreateParams::ServiceTier, nil]
        optional :service_tier, enum: -> { OpenAI::Models::Chat::CompletionCreateParams::ServiceTier }, nil?: true

        # @!attribute stop
        #   Up to 4 sequences where the API will stop generating further tokens. The
        #     returned text will not contain the stop sequence.
        #
        #   @return [String, Array<String>, nil]
        optional :stop, union: -> { OpenAI::Models::Chat::CompletionCreateParams::Stop }, nil?: true

        # @!attribute store
        #   Whether or not to store the output of this chat completion request for use in
        #     our [model distillation](https://platform.openai.com/docs/guides/distillation)
        #     or [evals](https://platform.openai.com/docs/guides/evals) products.
        #
        #   @return [Boolean, nil]
        optional :store, OpenAI::BooleanModel, nil?: true

        # @!attribute stream_options
        #   Options for streaming response. Only set this when you set `stream: true`.
        #
        #   @return [OpenAI::Models::Chat::ChatCompletionStreamOptions, nil]
        optional :stream_options, -> { OpenAI::Models::Chat::ChatCompletionStreamOptions }, nil?: true

        # @!attribute temperature
        #   What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
        #     make the output more random, while lower values like 0.2 will make it more
        #     focused and deterministic. We generally recommend altering this or `top_p` but
        #     not both.
        #
        #   @return [Float, nil]
        optional :temperature, Float, nil?: true

        # @!attribute [r] tool_choice
        #   Controls which (if any) tool is called by the model. `none` means the model will
        #     not call any tool and instead generates a message. `auto` means the model can
        #     pick between generating a message or calling one or more tools. `required` means
        #     the model must call one or more tools. Specifying a particular tool via
        #     `{"type": "function", "function": {"name": "my_function"}}` forces the model to
        #     call that tool.
        #
        #     `none` is the default when no tools are present. `auto` is the default if tools
        #     are present.
        #
        #   @return [Symbol, OpenAI::Models::Chat::ChatCompletionToolChoiceOption::Auto, OpenAI::Models::Chat::ChatCompletionNamedToolChoice, nil]
        optional :tool_choice, union: -> { OpenAI::Models::Chat::ChatCompletionToolChoiceOption }

        # @!parse
        #   # @return [Symbol, OpenAI::Models::Chat::ChatCompletionToolChoiceOption::Auto, OpenAI::Models::Chat::ChatCompletionNamedToolChoice]
        #   attr_writer :tool_choice

        # @!attribute [r] tools
        #   A list of tools the model may call. Currently, only functions are supported as a
        #     tool. Use this to provide a list of functions the model may generate JSON inputs
        #     for. A max of 128 functions are supported.
        #
        #   @return [Array<OpenAI::Models::Chat::ChatCompletionTool>, nil]
        optional :tools, -> { OpenAI::ArrayOf[OpenAI::Models::Chat::ChatCompletionTool] }

        # @!parse
        #   # @return [Array<OpenAI::Models::Chat::ChatCompletionTool>]
        #   attr_writer :tools

        # @!attribute top_logprobs
        #   An integer between 0 and 20 specifying the number of most likely tokens to
        #     return at each token position, each with an associated log probability.
        #     `logprobs` must be set to `true` if this parameter is used.
        #
        #   @return [Integer, nil]
        optional :top_logprobs, Integer, nil?: true

        # @!attribute top_p
        #   An alternative to sampling with temperature, called nucleus sampling, where the
        #     model considers the results of the tokens with top_p probability mass. So 0.1
        #     means only the tokens comprising the top 10% probability mass are considered.
        #
        #     We generally recommend altering this or `temperature` but not both.
        #
        #   @return [Float, nil]
        optional :top_p, Float, nil?: true

        # @!attribute [r] user
        #   A unique identifier representing your end-user, which can help OpenAI to monitor
        #     and detect abuse.
        #     [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
        #
        #   @return [String, nil]
        optional :user, String

        # @!parse
        #   # @return [String]
        #   attr_writer :user

        # @!attribute [r] web_search_options
        #   This tool searches the web for relevant results to use in a response. Learn more
        #     about the
        #     [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
        #
        #   @return [OpenAI::Models::Chat::CompletionCreateParams::WebSearchOptions, nil]
        optional :web_search_options, -> { OpenAI::Models::Chat::CompletionCreateParams::WebSearchOptions }

        # @!parse
        #   # @return [OpenAI::Models::Chat::CompletionCreateParams::WebSearchOptions]
        #   attr_writer :web_search_options

        # @!parse
        #   # @param messages [Array<OpenAI::Models::Chat::ChatCompletionDeveloperMessageParam, OpenAI::Models::Chat::ChatCompletionSystemMessageParam, OpenAI::Models::Chat::ChatCompletionUserMessageParam, OpenAI::Models::Chat::ChatCompletionAssistantMessageParam, OpenAI::Models::Chat::ChatCompletionToolMessageParam, OpenAI::Models::Chat::ChatCompletionFunctionMessageParam>]
        #   # @param model [String, Symbol, OpenAI::Models::ChatModel]
        #   # @param audio [OpenAI::Models::Chat::ChatCompletionAudioParam, nil]
        #   # @param frequency_penalty [Float, nil]
        #   # @param function_call [Symbol, OpenAI::Models::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode, OpenAI::Models::Chat::ChatCompletionFunctionCallOption]
        #   # @param functions [Array<OpenAI::Models::Chat::CompletionCreateParams::Function>]
        #   # @param logit_bias [Hash{Symbol=>Integer}, nil]
        #   # @param logprobs [Boolean, nil]
        #   # @param max_completion_tokens [Integer, nil]
        #   # @param max_tokens [Integer, nil]
        #   # @param metadata [Hash{Symbol=>String}, nil]
        #   # @param modalities [Array<Symbol, OpenAI::Models::Chat::CompletionCreateParams::Modality>, nil]
        #   # @param n [Integer, nil]
        #   # @param parallel_tool_calls [Boolean]
        #   # @param prediction [OpenAI::Models::Chat::ChatCompletionPredictionContent, nil]
        #   # @param presence_penalty [Float, nil]
        #   # @param reasoning_effort [Symbol, OpenAI::Models::ReasoningEffort, nil]
        #   # @param response_format [OpenAI::Models::ResponseFormatText, OpenAI::Models::ResponseFormatJSONSchema, OpenAI::Models::ResponseFormatJSONObject]
        #   # @param seed [Integer, nil]
        #   # @param service_tier [Symbol, OpenAI::Models::Chat::CompletionCreateParams::ServiceTier, nil]
        #   # @param stop [String, Array<String>, nil]
        #   # @param store [Boolean, nil]
        #   # @param stream_options [OpenAI::Models::Chat::ChatCompletionStreamOptions, nil]
        #   # @param temperature [Float, nil]
        #   # @param tool_choice [Symbol, OpenAI::Models::Chat::ChatCompletionToolChoiceOption::Auto, OpenAI::Models::Chat::ChatCompletionNamedToolChoice]
        #   # @param tools [Array<OpenAI::Models::Chat::ChatCompletionTool>]
        #   # @param top_logprobs [Integer, nil]
        #   # @param top_p [Float, nil]
        #   # @param user [String]
        #   # @param web_search_options [OpenAI::Models::Chat::CompletionCreateParams::WebSearchOptions]
        #   # @param request_options [OpenAI::RequestOptions, Hash{Symbol=>Object}]
        #   #
        #   def initialize(
        #     messages:,
        #     model:,
        #     audio: nil,
        #     frequency_penalty: nil,
        #     function_call: nil,
        #     functions: nil,
        #     logit_bias: nil,
        #     logprobs: nil,
        #     max_completion_tokens: nil,
        #     max_tokens: nil,
        #     metadata: nil,
        #     modalities: nil,
        #     n: nil,
        #     parallel_tool_calls: nil,
        #     prediction: nil,
        #     presence_penalty: nil,
        #     reasoning_effort: nil,
        #     response_format: nil,
        #     seed: nil,
        #     service_tier: nil,
        #     stop: nil,
        #     store: nil,
        #     stream_options: nil,
        #     temperature: nil,
        #     tool_choice: nil,
        #     tools: nil,
        #     top_logprobs: nil,
        #     top_p: nil,
        #     user: nil,
        #     web_search_options: nil,
        #     request_options: {},
        #     **
        #   )
        #     super
        #   end

        # def initialize: (Hash | OpenAI::BaseModel) -> void

        # Model ID used to generate the response, like `gpt-4o` or `o1`. OpenAI offers a
        #   wide range of models with different capabilities, performance characteristics,
        #   and price points. Refer to the
        #   [model guide](https://platform.openai.com/docs/models) to browse and compare
        #   available models.
        module Model
          extend OpenAI::Union

          variant String

          # Model ID used to generate the response, like `gpt-4o` or `o1`. OpenAI
          # offers a wide range of models with different capabilities, performance
          # characteristics, and price points. Refer to the [model guide](https://platform.openai.com/docs/models)
          # to browse and compare available models.
          variant enum: -> { OpenAI::Models::ChatModel }

          # @!parse
          #   # @return [Array(String, Symbol, OpenAI::Models::ChatModel)]
          #   def self.variants; end
        end

        # @deprecated
        #
        # Deprecated in favor of `tool_choice`.
        #
        #   Controls which (if any) function is called by the model.
        #
        #   `none` means the model will not call a function and instead generates a message.
        #
        #   `auto` means the model can pick between generating a message or calling a
        #   function.
        #
        #   Specifying a particular function via `{"name": "my_function"}` forces the model
        #   to call that function.
        #
        #   `none` is the default when no functions are present. `auto` is the default if
        #   functions are present.
        module FunctionCall
          extend OpenAI::Union

          # `none` means the model will not call a function and instead generates a message. `auto` means the model can pick between generating a message or calling a function.
          variant enum: -> { OpenAI::Models::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode }

          # Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.
          variant -> { OpenAI::Models::Chat::ChatCompletionFunctionCallOption }

          # `none` means the model will not call a function and instead generates a message.
          #   `auto` means the model can pick between generating a message or calling a
          #   function.
          module FunctionCallMode
            extend OpenAI::Enum

            NONE = :none
            AUTO = :auto

            finalize!

            # @!parse
            #   # @return [Array<Symbol>]
            #   def self.values; end
          end

          # @!parse
          #   # @return [Array(Symbol, OpenAI::Models::Chat::CompletionCreateParams::FunctionCall::FunctionCallMode, OpenAI::Models::Chat::ChatCompletionFunctionCallOption)]
          #   def self.variants; end
        end

        # @deprecated
        class Function < OpenAI::BaseModel
          # @!attribute name
          #   The name of the function to be called. Must be a-z, A-Z, 0-9, or contain
          #     underscores and dashes, with a maximum length of 64.
          #
          #   @return [String]
          required :name, String

          # @!attribute [r] description
          #   A description of what the function does, used by the model to choose when and
          #     how to call the function.
          #
          #   @return [String, nil]
          optional :description, String

          # @!parse
          #   # @return [String]
          #   attr_writer :description

          # @!attribute [r] parameters
          #   The parameters the functions accepts, described as a JSON Schema object. See the
          #     [guide](https://platform.openai.com/docs/guides/function-calling) for examples,
          #     and the
          #     [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for
          #     documentation about the format.
          #
          #     Omitting `parameters` defines a function with an empty parameter list.
          #
          #   @return [Hash{Symbol=>Object}, nil]
          optional :parameters, OpenAI::HashOf[OpenAI::Unknown]

          # @!parse
          #   # @return [Hash{Symbol=>Object}]
          #   attr_writer :parameters

          # @!parse
          #   # @param name [String]
          #   # @param description [String]
          #   # @param parameters [Hash{Symbol=>Object}]
          #   #
          #   def initialize(name:, description: nil, parameters: nil, **) = super

          # def initialize: (Hash | OpenAI::BaseModel) -> void
        end

        module Modality
          extend OpenAI::Enum

          TEXT = :text
          AUDIO = :audio

          finalize!

          # @!parse
          #   # @return [Array<Symbol>]
          #   def self.values; end
        end

        # An object specifying the format that the model must output.
        #
        #   Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
        #   Outputs which ensures the model will match your supplied JSON schema. Learn more
        #   in the
        #   [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
        #
        #   Setting to `{ "type": "json_object" }` enables the older JSON mode, which
        #   ensures the message the model generates is valid JSON. Using `json_schema` is
        #   preferred for models that support it.
        module ResponseFormat
          extend OpenAI::Union

          # Default response format. Used to generate text responses.
          variant -> { OpenAI::Models::ResponseFormatText }

          # JSON Schema response format. Used to generate structured JSON responses.
          # Learn more about [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs).
          variant -> { OpenAI::Models::ResponseFormatJSONSchema }

          # JSON object response format. An older method of generating JSON responses.
          # Using `json_schema` is recommended for models that support it. Note that the
          # model will not generate JSON without a system or user message instructing it
          # to do so.
          variant -> { OpenAI::Models::ResponseFormatJSONObject }

          # @!parse
          #   # @return [Array(OpenAI::Models::ResponseFormatText, OpenAI::Models::ResponseFormatJSONSchema, OpenAI::Models::ResponseFormatJSONObject)]
          #   def self.variants; end
        end

        # Specifies the latency tier to use for processing the request. This parameter is
        #   relevant for customers subscribed to the scale tier service:
        #
        #   - If set to 'auto', and the Project is Scale tier enabled, the system will
        #     utilize scale tier credits until they are exhausted.
        #   - If set to 'auto', and the Project is not Scale tier enabled, the request will
        #     be processed using the default service tier with a lower uptime SLA and no
        #     latency guarentee.
        #   - If set to 'default', the request will be processed using the default service
        #     tier with a lower uptime SLA and no latency guarentee.
        #   - When not set, the default behavior is 'auto'.
        #
        #   When this parameter is set, the response body will include the `service_tier`
        #   utilized.
        module ServiceTier
          extend OpenAI::Enum

          AUTO = :auto
          DEFAULT = :default

          finalize!

          # @!parse
          #   # @return [Array<Symbol>]
          #   def self.values; end
        end

        # Up to 4 sequences where the API will stop generating further tokens. The
        #   returned text will not contain the stop sequence.
        module Stop
          extend OpenAI::Union

          StringArray = OpenAI::ArrayOf[String]

          variant String

          variant OpenAI::Models::Chat::CompletionCreateParams::Stop::StringArray

          # @!parse
          #   # @return [Array(String, Array<String>)]
          #   def self.variants; end
        end

        class WebSearchOptions < OpenAI::BaseModel
          # @!attribute [r] search_context_size
          #   High level guidance for the amount of context window space to use for the
          #     search. One of `low`, `medium`, or `high`. `medium` is the default.
          #
          #   @return [Symbol, OpenAI::Models::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize, nil]
          optional :search_context_size,
                   enum: -> { OpenAI::Models::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize }

          # @!parse
          #   # @return [Symbol, OpenAI::Models::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize]
          #   attr_writer :search_context_size

          # @!attribute user_location
          #   Approximate location parameters for the search.
          #
          #   @return [OpenAI::Models::Chat::CompletionCreateParams::WebSearchOptions::UserLocation, nil]
          optional :user_location,
                   -> { OpenAI::Models::Chat::CompletionCreateParams::WebSearchOptions::UserLocation },
                   nil?: true

          # @!parse
          #   # This tool searches the web for relevant results to use in a response. Learn more
          #   #   about the
          #   #   [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
          #   #
          #   # @param search_context_size [Symbol, OpenAI::Models::Chat::CompletionCreateParams::WebSearchOptions::SearchContextSize]
          #   # @param user_location [OpenAI::Models::Chat::CompletionCreateParams::WebSearchOptions::UserLocation, nil]
          #   #
          #   def initialize(search_context_size: nil, user_location: nil, **) = super

          # def initialize: (Hash | OpenAI::BaseModel) -> void

          # High level guidance for the amount of context window space to use for the
          #   search. One of `low`, `medium`, or `high`. `medium` is the default.
          module SearchContextSize
            extend OpenAI::Enum

            LOW = :low
            MEDIUM = :medium
            HIGH = :high

            finalize!

            # @!parse
            #   # @return [Array<Symbol>]
            #   def self.values; end
          end

          class UserLocation < OpenAI::BaseModel
            # @!attribute approximate
            #   Approximate location parameters for the search.
            #
            #   @return [OpenAI::Models::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::Approximate]
            required :approximate,
                     -> { OpenAI::Models::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::Approximate }

            # @!attribute type
            #   The type of location approximation. Always `approximate`.
            #
            #   @return [Symbol, :approximate]
            required :type, const: :approximate

            # @!parse
            #   # Approximate location parameters for the search.
            #   #
            #   # @param approximate [OpenAI::Models::Chat::CompletionCreateParams::WebSearchOptions::UserLocation::Approximate]
            #   # @param type [Symbol, :approximate]
            #   #
            #   def initialize(approximate:, type: :approximate, **) = super

            # def initialize: (Hash | OpenAI::BaseModel) -> void

            class Approximate < OpenAI::BaseModel
              # @!attribute [r] city
              #   Free text input for the city of the user, e.g. `San Francisco`.
              #
              #   @return [String, nil]
              optional :city, String

              # @!parse
              #   # @return [String]
              #   attr_writer :city

              # @!attribute [r] country
              #   The two-letter [ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of
              #     the user, e.g. `US`.
              #
              #   @return [String, nil]
              optional :country, String

              # @!parse
              #   # @return [String]
              #   attr_writer :country

              # @!attribute [r] region
              #   Free text input for the region of the user, e.g. `California`.
              #
              #   @return [String, nil]
              optional :region, String

              # @!parse
              #   # @return [String]
              #   attr_writer :region

              # @!attribute [r] timezone
              #   The [IANA timezone](https://timeapi.io/documentation/iana-timezones) of the
              #     user, e.g. `America/Los_Angeles`.
              #
              #   @return [String, nil]
              optional :timezone, String

              # @!parse
              #   # @return [String]
              #   attr_writer :timezone

              # @!parse
              #   # Approximate location parameters for the search.
              #   #
              #   # @param city [String]
              #   # @param country [String]
              #   # @param region [String]
              #   # @param timezone [String]
              #   #
              #   def initialize(city: nil, country: nil, region: nil, timezone: nil, **) = super

              # def initialize: (Hash | OpenAI::BaseModel) -> void
            end
          end
        end
      end
    end
  end
end
